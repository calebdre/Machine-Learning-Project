{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from time import time\n",
    "from math import sqrt, floor\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "pd.options.display.float_format = '{:.3f}'.format\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use = \"default\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.debugger import set_trace\n",
    "train = pd.read_csv(\"cleaned_testData1.csv\")\n",
    "labels = pd.read_csv(\"cleaned_trainLabel1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.drop(train.columns[0], axis=1)\n",
    "labels = labels.drop(labels.columns[0], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(df, labels):\n",
    "    return labels.merge(df, left_index=True,right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---  \n",
    "\n",
    "# Model Building\n",
    "\n",
    "## Which algorithm to use?\n",
    "We'll use a random forest classifier (rfc) with bootstrapping and feature bagging optimizations because:\n",
    "- ease of implementation\n",
    "- rfcs handle multi-class predictions well without more additional effort\n",
    "- works well with high dimensional data\n",
    "- we'll choose use random forest as opposed to boosted trees since we have highly dimensional data\n",
    "- with a reasonably high probability, can be used with the other datasets for this project since the algorithm is very robust\n",
    "\n",
    "## The Algorithm\n",
    "We'll use the CART algorithm for splitting since we have continuous data.  \n",
    "  \n",
    "[Full example](https://machinelearningmastery.com/classification-and-regression-trees-for-machine-learning/)  \n",
    "  \n",
    "Steps:\n",
    "1. Initialize Tree\n",
    "2. For each column, calc best split across all rows based using gini impurity score - [exmplanation](https://en.wikipedia.org/wiki/Decision_tree_learning#Gini_impurity) | [exmaple](https://www.researchgate.net/post/How_to_compute_impurity_using_Gini_Index) | [useful blog](http://dni-institute.in/blogs/cart-algorithm-for-decision-tree/)\n",
    "3. Split the dataset based on the split condition with the highest gini score and add both sets as leaves on a tree node. The node represents a decision point, that being the condition with the highest gini score.\n",
    "3. Repeat 2 & 3 until an arbitrary minimum number of rows are left\n",
    "4. Prune tree\n",
    "\n",
    "ideas:\n",
    "- instead of using the raw values, categorize the numbers as # of stds away from mean\n",
    "- > Alternatively, the random forest can apply weight concept for considering the impact of result from any decision tree. Tree with high error rate are given low weight value and vise versa. This would increase the decision impact of trees with low error rate - [medium post](https://medium.com/machine-learning-101/chapter-5-random-forest-classifier-56dc7425c3e1)\n",
    "- [parameters to  tune](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)\n",
    "https://www.analyticsvidhya.com/blog/2016/04/complete-tutorial-tree-based-modeling-scratch-in-python/\n",
    "- https://www.analyticsvidhya.com/blog/2015/06/tuning-random-forest-model/\n",
    "- https://stats.stackexchange.com/questions/260460/optimization-of-a-random-forest-model\n",
    "- https://followthedata.wordpress.com/2012/06/02/practical-advice-for-machine-learning-bias-variance/\n",
    "- https://towardsdatascience.com/beyond-accuracy-precision-and-recall-3da06bea9f6c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TEST\n",
    "# def calc_best_gini_split(df, labels):\n",
    "#     total_rows = df.shape[0]\n",
    "    \n",
    "#     def calc(col_df):\n",
    "#         def g(item):\n",
    "#             split1 = col_df[item > col_df]\n",
    "#             split2 = col_df[item < col_df]\n",
    "            \n",
    "#             s1_grouped = labels.merge(pd.DataFrame(split1), left_index=True, right_index=True).groupby(\"label\")\n",
    "#             s2_grouped = labels.merge(pd.DataFrame(split2), left_index=True, right_index=True).groupby(\"label\")\n",
    "            \n",
    "#             s1_group_count = s1_grouped.count()\n",
    "#             s2_group_count = s2_grouped.count()\n",
    "            \n",
    "#             s1_cost = (1 - np.power(s1_group_count / s1_group_count.sum(), 2).sum(axis=0)) * (split1.shape[0] / total_rows)\n",
    "#             s2_cost = (1 - np.power(s2_group_count / s2_group_count.sum(), 2).sum(axis=0)) * (split2.shape[0] / total_rows)\n",
    "\n",
    "#             return (s1_cost + s2_cost).iloc[0]\n",
    "        \n",
    "#         # minimum cost for particular columns        \n",
    "#         costs = col_df.apply(g)\n",
    "#         min_cost = costs.min()\n",
    "#         print(costs.shape, costs.idxmin(), costs, df)\n",
    "#         min_cost_val = costs.iloc[costs.idxmin()]\n",
    "        \n",
    "#         return pd.Series({\"cost\": min_cost, \"column\": col_df.name, \"val\": min_cost_val})\n",
    "    \n",
    "#     splits = df.apply(calc, axis=0)\n",
    "    \n",
    "#     # minimum cost for whole set\n",
    "#     transposed = splits.T\n",
    "#     transposed[\"cost\"] = transposed[\"cost\"].astype(np.float32)\n",
    "#     best_split = splits[transposed[\"cost\"].idxmin()]\n",
    "    \n",
    "#     return best_split[\"column\"], best_split[\"val\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t = time()\n",
    "# calc_best_gini_split(train.iloc[:, 0:70], labels)\n",
    "# time() - t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_best_gini_split(df, labels):\n",
    "    total_rows = df.shape[0]\n",
    "    \n",
    "    def calc(col_df):\n",
    "        min_cost = 2\n",
    "        min_cost_index = -1\n",
    "        for index, row in col_df.iteritems():\n",
    "            split1 = col_df[row > col_df]\n",
    "            split2 = col_df[row < col_df]\n",
    "\n",
    "            s1_grouped = labels.merge(pd.DataFrame(split1), left_index=True, right_index=True).groupby(\"label\")\n",
    "            s2_grouped = labels.merge(pd.DataFrame(split2), left_index=True, right_index=True).groupby(\"label\")\n",
    "\n",
    "            s1_group_count = s1_grouped.count()\n",
    "            s2_group_count = s2_grouped.count()\n",
    "            \n",
    "            s1_cost = (1 - np.power(s1_group_count / s1_group_count.sum(), 2).sum(axis=0)) * (split1.shape[0] / total_rows)\n",
    "            s2_cost = (1 - np.power(s2_group_count / s2_group_count.sum(), 2).sum(axis=0)) * (split2.shape[0] / total_rows)\n",
    "\n",
    "            total_cost = (s1_cost + s2_cost).iloc[0]\n",
    "            if total_cost < min_cost:\n",
    "                min_cost = total_cost\n",
    "                min_cost_index = index\n",
    "        return pd.Series({\"cost\": min_cost, \"index\": min_cost_index})\n",
    "    \n",
    "    splits = df.apply(calc, axis=0)\n",
    "    best_split_col = splits.T[\"cost\"].idxmin()\n",
    "    best_split_index = splits[best_split_col][\"index\"]\n",
    "    \n",
    "    return best_split_col, df[best_split_col][best_split_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t = time()\n",
    "# calc_best_gini_split(train.iloc[:, 0:70], labels)\n",
    "# time() - t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, column, value):\n",
    "        self.col = column\n",
    "        self.val = value\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "    \n",
    "    def traverse(self, row):\n",
    "        if row[self.col] > self.val:\n",
    "            return self.left\n",
    "        else:\n",
    "            return self.right\n",
    "\n",
    "class DecisionTree:\n",
    "    def __init__(self):\n",
    "        self.root = None\n",
    "    \n",
    "    def grow(self, origin_df, labels, max_depth = 20, min_split_samples = 10):\n",
    "        def classify_branch(df, labels):\n",
    "            return labels.merge(df, left_index=True, right_index=True).groupby(\"label\").count().sum(axis=1).idxmax()\n",
    "        \n",
    "        def grow_recurse(df, labels, depth):\n",
    "            print(\"parsing @ depth {}\".format(depth))\n",
    "            col, val = calc_best_gini_split(df, labels)\n",
    "            left_split = df[df[col] > val]\n",
    "            right_split = df[df[col] < val]\n",
    "\n",
    "            if depth > max_depth or np.minimum(left_split.shape[0], right_split.shape[0]) < min_split_samples:\n",
    "                classification = classify_branch(df, labels)\n",
    "                print(\"classified {} at depth {} to be in class {}\".format(col, depth, classification))\n",
    "                return classification\n",
    "\n",
    "            node = Node(col, val)\n",
    "            node.left = grow_recurse(left_split, labels, depth + 1)\n",
    "            node.right = grow_recurse(right_split, labels, depth + 1)\n",
    "\n",
    "            print(\"Done with depth {}\".format(depth))\n",
    "            return node\n",
    "        \n",
    "        self.root = grow_recurse(origin_df, labels, 1)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, row):\n",
    "        if self.root is None: \n",
    "            raise Exception(\"Must call `grow` before using `predict`\")\n",
    "        \n",
    "        node = self.root\n",
    "        while type(node) is not np.int64:\n",
    "            node = self.root.traverse(row)\n",
    "        \n",
    "        return node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsing @ depth 1\n",
      "parsing @ depth 2\n",
      "classified f4 at depth 2 to be in class 4\n",
      "parsing @ depth 2\n",
      "classified f7 at depth 2 to be in class 1\n",
      "Done with depth 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, label    1\n",
       " Name: 144, dtype: int64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_row = -5\n",
    "t = train.iloc[:, 0:15]\n",
    "\n",
    "DecisionTree().grow(t, labels).predict(t.iloc[test_row]), labels.iloc[-5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForest:\n",
    "    def __init__(self):\n",
    "        self.trees = []\n",
    "    \n",
    "    def grow(self, origin_df, labels, num_trees=10, num_features=None, num_sample_rows=None , max_tree_depth=20, min_split_samples=5):\n",
    "        if num_features is None:\n",
    "            num_features = floor(sqrt(origin_df.columns.size))\n",
    "        if num_sample_rows is None:\n",
    "            num_sample_rows = floor(origin_df.shape[0] / 4)\n",
    "\n",
    "        for i in range(num_trees):\n",
    "            features = np.random.choice(origin_df.columns, size=num_features, replace=False)\n",
    "            rows = np.random.choice(origin_df.shape[0], size=num_sample_rows, replace=False)\n",
    "\n",
    "            df = origin_df[features]\n",
    "            df = df.iloc[rows]\n",
    "\n",
    "            print(\"\\n*** Creating tree #{} ***\\n\".format(i+1))\n",
    "            self.trees.append(DecisionTree().grow(df, labels, max_tree_depth, min_split_samples))\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def predict(self, row):\n",
    "        if not isinstance(row, pd.Series):\n",
    "            raise Exception(\"`row` must be an instance of Pandas.Series\")\n",
    "            \n",
    "        predictions = [tree.predict(row) for tree in self.trees]\n",
    "        return np.bincount(predictions).argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Creating tree #1 ***\n",
      "\n",
      "parsing @ depth 1\n",
      "parsing @ depth 2\n",
      "classified f35 at depth 2 to be in class 4\n",
      "parsing @ depth 2\n",
      "parsing @ depth 3\n",
      "classified f35 at depth 3 to be in class 1\n",
      "parsing @ depth 3\n",
      "parsing @ depth 4\n",
      "parsing @ depth 5\n",
      "classified f35 at depth 5 to be in class 1\n",
      "parsing @ depth 5\n",
      "classified f35 at depth 5 to be in class 1\n",
      "Done with depth 4\n",
      "parsing @ depth 4\n",
      "classified f35 at depth 4 to be in class 1\n",
      "Done with depth 3\n",
      "Done with depth 2\n",
      "Done with depth 1\n",
      "\n",
      "*** Creating tree #2 ***\n",
      "\n",
      "parsing @ depth 1\n",
      "classified f23 at depth 1 to be in class 1\n",
      "\n",
      "*** Creating tree #3 ***\n",
      "\n",
      "parsing @ depth 1\n",
      "classified f25 at depth 1 to be in class 1\n",
      "\n",
      "*** Creating tree #4 ***\n",
      "\n",
      "parsing @ depth 1\n",
      "classified f25 at depth 1 to be in class 1\n",
      "\n",
      "*** Creating tree #5 ***\n",
      "\n",
      "parsing @ depth 1\n",
      "classified f20 at depth 1 to be in class 1\n",
      "\n",
      "*** Creating tree #6 ***\n",
      "\n",
      "parsing @ depth 1\n",
      "classified f45 at depth 1 to be in class 1\n",
      "\n",
      "*** Creating tree #7 ***\n",
      "\n",
      "parsing @ depth 1\n",
      "classified f42 at depth 1 to be in class 1\n",
      "\n",
      "*** Creating tree #8 ***\n",
      "\n",
      "parsing @ depth 1\n",
      "classified f18 at depth 1 to be in class 1\n",
      "\n",
      "*** Creating tree #9 ***\n",
      "\n",
      "parsing @ depth 1\n",
      "classified f27 at depth 1 to be in class 1\n",
      "\n",
      "*** Creating tree #10 ***\n",
      "\n",
      "parsing @ depth 1\n",
      "classified f20 at depth 1 to be in class 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.RandomForest at 0x10ff6da90>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dft = train.iloc[:, 0:50]\n",
    "rf = RandomForest()\n",
    "rf.grow(dft, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, label    1\n",
       " Name: 144, dtype: int64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_row = -5\n",
    "rf.predict(train.iloc[test_row]), labels.iloc[test_row]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"8\" halign=\"left\">f0</th>\n",
       "      <th colspan=\"2\" halign=\"left\">f1</th>\n",
       "      <th>...</th>\n",
       "      <th colspan=\"2\" halign=\"left\">f8</th>\n",
       "      <th colspan=\"8\" halign=\"left\">f9</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>...</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>107.000</td>\n",
       "      <td>1.674</td>\n",
       "      <td>0.215</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.578</td>\n",
       "      <td>1.689</td>\n",
       "      <td>1.813</td>\n",
       "      <td>2.196</td>\n",
       "      <td>107.000</td>\n",
       "      <td>2.356</td>\n",
       "      <td>...</td>\n",
       "      <td>1.940</td>\n",
       "      <td>2.304</td>\n",
       "      <td>107.000</td>\n",
       "      <td>2.478</td>\n",
       "      <td>0.222</td>\n",
       "      <td>1.709</td>\n",
       "      <td>2.363</td>\n",
       "      <td>2.499</td>\n",
       "      <td>2.648</td>\n",
       "      <td>2.853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14.000</td>\n",
       "      <td>1.510</td>\n",
       "      <td>0.186</td>\n",
       "      <td>1.211</td>\n",
       "      <td>1.380</td>\n",
       "      <td>1.478</td>\n",
       "      <td>1.604</td>\n",
       "      <td>1.893</td>\n",
       "      <td>14.000</td>\n",
       "      <td>2.386</td>\n",
       "      <td>...</td>\n",
       "      <td>1.399</td>\n",
       "      <td>1.640</td>\n",
       "      <td>14.000</td>\n",
       "      <td>2.386</td>\n",
       "      <td>0.242</td>\n",
       "      <td>1.819</td>\n",
       "      <td>2.360</td>\n",
       "      <td>2.402</td>\n",
       "      <td>2.522</td>\n",
       "      <td>2.726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.000</td>\n",
       "      <td>1.480</td>\n",
       "      <td>0.265</td>\n",
       "      <td>1.042</td>\n",
       "      <td>1.383</td>\n",
       "      <td>1.494</td>\n",
       "      <td>1.637</td>\n",
       "      <td>1.840</td>\n",
       "      <td>11.000</td>\n",
       "      <td>2.520</td>\n",
       "      <td>...</td>\n",
       "      <td>1.949</td>\n",
       "      <td>2.109</td>\n",
       "      <td>11.000</td>\n",
       "      <td>2.401</td>\n",
       "      <td>0.412</td>\n",
       "      <td>1.366</td>\n",
       "      <td>2.411</td>\n",
       "      <td>2.511</td>\n",
       "      <td>2.618</td>\n",
       "      <td>2.811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14.000</td>\n",
       "      <td>2.180</td>\n",
       "      <td>0.246</td>\n",
       "      <td>1.745</td>\n",
       "      <td>2.014</td>\n",
       "      <td>2.128</td>\n",
       "      <td>2.375</td>\n",
       "      <td>2.579</td>\n",
       "      <td>14.000</td>\n",
       "      <td>2.534</td>\n",
       "      <td>...</td>\n",
       "      <td>2.148</td>\n",
       "      <td>2.434</td>\n",
       "      <td>14.000</td>\n",
       "      <td>2.921</td>\n",
       "      <td>0.163</td>\n",
       "      <td>2.652</td>\n",
       "      <td>2.812</td>\n",
       "      <td>2.928</td>\n",
       "      <td>3.052</td>\n",
       "      <td>3.155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3.000</td>\n",
       "      <td>1.930</td>\n",
       "      <td>0.189</td>\n",
       "      <td>1.715</td>\n",
       "      <td>1.862</td>\n",
       "      <td>2.009</td>\n",
       "      <td>2.038</td>\n",
       "      <td>2.067</td>\n",
       "      <td>3.000</td>\n",
       "      <td>2.385</td>\n",
       "      <td>...</td>\n",
       "      <td>2.099</td>\n",
       "      <td>2.119</td>\n",
       "      <td>3.000</td>\n",
       "      <td>2.593</td>\n",
       "      <td>0.137</td>\n",
       "      <td>2.511</td>\n",
       "      <td>2.514</td>\n",
       "      <td>2.516</td>\n",
       "      <td>2.634</td>\n",
       "      <td>2.751</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 400 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           f0                                                f1        ...   \\\n",
       "        count  mean   std   min   25%   50%   75%   max   count  mean  ...    \n",
       "label                                                                  ...    \n",
       "1     107.000 1.674 0.215 1.000 1.578 1.689 1.813 2.196 107.000 2.356  ...    \n",
       "2      14.000 1.510 0.186 1.211 1.380 1.478 1.604 1.893  14.000 2.386  ...    \n",
       "3      11.000 1.480 0.265 1.042 1.383 1.494 1.637 1.840  11.000 2.520  ...    \n",
       "4      14.000 2.180 0.246 1.745 2.014 2.128 2.375 2.579  14.000 2.534  ...    \n",
       "5       3.000 1.930 0.189 1.715 1.862 2.009 2.038 2.067   3.000 2.385  ...    \n",
       "\n",
       "         f8            f9                                            \n",
       "        75%   max   count  mean   std   min   25%   50%   75%   max  \n",
       "label                                                                \n",
       "1     1.940 2.304 107.000 2.478 0.222 1.709 2.363 2.499 2.648 2.853  \n",
       "2     1.399 1.640  14.000 2.386 0.242 1.819 2.360 2.402 2.522 2.726  \n",
       "3     1.949 2.109  11.000 2.401 0.412 1.366 2.411 2.511 2.618 2.811  \n",
       "4     2.148 2.434  14.000 2.921 0.163 2.652 2.812 2.928 3.052 3.155  \n",
       "5     2.099 2.119   3.000 2.593 0.137 2.511 2.514 2.516 2.634 2.751  \n",
       "\n",
       "[5 rows x 400 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge(dft, labels).groupby(\"label\").describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---  \n",
    "# Model Training\n",
    "## Context\n",
    "Now that we have our classifier, let's think about how we're going to train the model. \n",
    "\n",
    "We'll also measure performance through [precision](https://en.wikipedia.org/wiki/Precision_and_recall) [recall](https://towardsdatascience.com/beyond-accuracy-precision-and-recall-3da06bea9f6c) - it tells us, for each class, how well the model identifies all cases of that class (recall) and how well it can correctly classify those cases (precision). From wikipedia:\n",
    "> Suppose a computer program for recognizing dogs in photographs identifies eight dogs in a picture containing 12 dogs and some cats. Of the eight dogs identified, five actually are dogs (true positives), while the rest are cats (false positives). The program's precision is 5/8 while its recall is 5/12.\n",
    "\n",
    "![precision & recall formulas](https://cdn-images-1.medium.com/max/2000/1*6NkN_LINs2erxgVJ9rkpUA.png)\n",
    "We can use the [f1 score](https://en.wikipedia.org/wiki/F1_score) to maximize precision and recall when testing different models.  \n",
    "![f1 score formula](https://cdn-images-1.medium.com/max/1600/1*UJxVqLnbSj42eRhasKeLOA.png)\n",
    "\n",
    "Recall and precision seem to be very related to bias and variance of the model, so we can maximize the f1 score by tuning the model to affect these.\n",
    "#### Minimizing bias\n",
    "- use new/different features\n",
    "- increase the size of the trees (increases variance)\n",
    "- increase the number of trees in the forest\n",
    "\n",
    "#### Minimizing variance\n",
    "- decrease the number of features\n",
    "    + probably want to aim to features that are correlated and/or collapse the overall number of features through PCA\n",
    "- use more data for each tree  \n",
    "\n",
    "  \n",
    "Beware: too much completixy is bad & not enough complexity is also bad  \n",
    "![bias variance tradeoff](http://scott.fortmann-roe.com/docs/docs/BiasVariance/biasvariance.png)  \n",
    "  \n",
    "\n",
    "#### Stability\n",
    "We need to make sure to train the classifier on as many data points as possible while also leaving enough to test to reliably tell how well the classifier actually performs. We'll use [k-fold cross validation](https://www.analyticsvidhya.com/blog/2015/11/improve-model-performance-cross-validation-in-python-r/):  \n",
    "  \n",
    "> 1. Randomly split your entire dataset into k ”folds”.\n",
    "2. For each k folds in your dataset, build your model on k – 1 folds of the data set. Then, test the model to check the effectiveness for kth fold.\n",
    "3. Record the error you see on each of the predictions.\n",
    "4. Repeat this until each of the k folds has served as the test set.\n",
    "\n",
    "## Procedure\n",
    "1. Record and save an input configuration for the random forest\n",
    "1. Separate data into k folds\n",
    "2. For each fold *k*: \n",
    "    1. train the classifier on k-1 folds\n",
    "    2. predict the k-th fold\n",
    "    3. measure the: accuracy, [logarithmic](http://wiki.fast.ai/index.php/Log_Loss) [loss](https://towardsdatascience.com/metrics-to-evaluate-your-machine-learning-algorithm-f10ba6e38234#f217), recall, precision, and f1-score\n",
    "3. Record the performance measures & associate it with the input configuration\n",
    "3. Evaluate the overall performance difference across all configurations\n",
    "4. Change **at most** 1 variable from the input configuration that optimizes perfomance & repeat steps 1-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_folds(df, k=10):\n",
    "    folds = [np.random.choice(range(df.shape[0]), 2) for i in range(k)]\n",
    "    return [df.iloc[np.amin(fold): np.amax(fold)] for fold in folds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = k_folds(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(50, 687),\n",
       " (29, 687),\n",
       " (53, 687),\n",
       " (118, 687),\n",
       " (65, 687),\n",
       " (24, 687),\n",
       " (23, 687),\n",
       " (46, 687),\n",
       " (38, 687),\n",
       " (21, 687)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i.shape for i in t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
