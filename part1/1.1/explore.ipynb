{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from time import time\n",
    "from math import sqrt, floor\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.core.debugger import set_trace\n",
    "from pandas.plotting import scatter_matrix\n",
    "pd.options.display.float_format = '{:.3f}'.format\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use = \"default\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(df, labels):\n",
    "    return labels.merge(df, left_index=True,right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"TestData1.txt\", sep='\\t', header=None)\n",
    "labels = pd.read_csv(\"TrainLabel1.txt\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "\n",
    "#### Todos:\n",
    "+ figure out how many and which rows for each column have these huge numbers\n",
    "+ figure out a way to map the values to ones that fit more with the data \n",
    "    * make them \"average joes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = train.iloc[:,0:10] # first row\n",
    "poten_errors = sample[sample > sample.mean()]\n",
    "poten_errors.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample[~(sample > sample.mean())].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~We have 69 rows with vastly diffrent values. We can't just drop the rows or columns because the signal to noise ratio for both axis is really low. Lets find out more about these errors.~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poten_errors.iloc[:, 0].dropna().plot.line()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like all the error numbers are the same even though the standard deviation, mean, and max are different numbers. Let's verify that the numbers are actually the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mMax = poten_errors.max()\n",
    "# all_diffs = [mMax - i for i in poten_errors]\n",
    "# extreme_diffs = poten_errors.max() - poten_errors.min()\n",
    "\n",
    "# print(\"Max - Min\\n\", extreme_diffs, \"\\nMax - All\\n\", all_diffs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since all the errors are indeed the same number, lets replace all of them with the mean of the particular column that they're in for now. We can think of a better way to replace those values later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in train.columns:\n",
    "    train_col = train[col]\n",
    "    gt_mean = train_col > train_col.mean()\n",
    "    new_col_mean = train_col[~gt_mean].mean()\n",
    "    \n",
    "    train[col][gt_mean] = new_col_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets find out how correlated each of these variabels are with the label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr(labels, train):\n",
    "    n = len(labels)\n",
    "    v1, v2 = labels.values, train.values\n",
    "    sums = np.multiply.outer(v2.sum(0), v1.sum(0))\n",
    "    stds = np.multiply.outer(v2.std(0), v1.std(0))\n",
    "    return pd.DataFrame((v2.T.dot(v1) - sums / n) / stds / n, train.columns, labels.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label_corr = corr(labels, train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_corrs(correlation_result, with_labels=True):\n",
    "    if with_labels:\n",
    "        return [(i, correlation_result[correlation_result > i].dropna().size) for i in np.arange(0,1.1,.1)]\n",
    "    else:\n",
    "        return [correlation_result[correlation_result > i].dropna().size for i in np.arange(0,1.1,.1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_corrs(train_label_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like there are lots of columns that aren't at all correlated with the labels. Let's drop all the ones who have a 20% or lower correlation. \n",
    "\n",
    "*idea: to improve the model, we could look into using the 20% category and play with corrlated columns in that category.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[train_label_corr[train_label_corr >= .5].dropna().index]\n",
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://sebastianraschka.com/Articles/2015_pca_in_3_steps.html\n",
    "# https://machinelearningmastery.com/calculate-principal-component-analysis-scratch-python/\n",
    "# https://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues\n",
    "train_corrs = train.corr()\n",
    "correlation_threshold = .85\n",
    "# threshold for dropping correlated columns\n",
    "high_corrs = {}\n",
    "for col in train_corrs:\n",
    "    df = train_corrs[col]\n",
    "    corrs = df[df >= correlation_threshold].dropna().keys().drop(col)\n",
    "    if corrs.size > 0:\n",
    "        high_corrs[col] = corrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_label_map = {}\n",
    "for i, col in enumerate(train.columns):\n",
    "    new_label_map[col] = \"f{}\".format(i)\n",
    "\n",
    "train = train.rename(columns=new_label_map)\n",
    "\n",
    "labels = labels.rename(columns={\"1\": \"label\"})\n",
    "merged = labels.merge(train, left_index=True,right_index=True)\n",
    "\n",
    "train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis\n",
    "- Are there a lot of outliers? If so, how many?\n",
    "- What is the relationship between each of the variables and the label? Linear? Parabolic?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.corr().stack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.corr().where([np.triu(np.ones(train.shape)).astype(np.bool)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard deviation is really good. Lets find outliers by getting the values that are `mean + std*n, n= {2 to 5}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_low_high = lambda mean, std, i: (df.mean() + (df.std()*float(i)), df.mean() + (df.std()*float(i+1)))\n",
    "\n",
    "v = {}\n",
    "for col in train:\n",
    "    df = train[col]\n",
    "    f = []\n",
    "    stds_from_min = np.floor((df.min() - df.mean()) / df.std())\n",
    "    for i in range(int(stds_from_min), 0):\n",
    "        low, high = calc_low_high(df.mean(), df.std(), i)\n",
    "        num_in_range = df[(low < df) & (df < high)].size\n",
    "        f.append((i, num_in_range))\n",
    "    \n",
    "    stds_from_max = (df.max() - df.mean()) / df.std()\n",
    "    for i in range(int(np.ceil(stds_from_max))):\n",
    "        low, high = calc_low_high(df.mean(), df.std(), i)\n",
    "        num_in_range = df[(low < df) & (df < high)].size\n",
    "        f.append((i, num_in_range))\n",
    "    v[col] = f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for col in train.columns:\n",
    "#     train[col].plot.kde()\n",
    "#     plt.legend()\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_dist = {}\n",
    "for key in v:\n",
    "    data = v[key]\n",
    "    for item in data:\n",
    "        if item[0] not in std_dist:\n",
    "            std_dist[item[0]] = [item[1]]\n",
    "        else:\n",
    "            std_dist[item[0]].append(item[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above shows, for each column in the training set, the number of values that fall into categories corresponding to the number of standard deveiations from the mean. \n",
    "The keys are the stds from mean and the values are the number of values that are that many stds from the mean found in each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(12,6))\n",
    "# for category in std_dist:\n",
    "#     plt.hist(std_dist[category], label=str(category))\n",
    "#     plt.legend()\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = np.array([(1,3), (4,5), (1,3), (4,5)])\n",
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([(1,3), (4,5)])[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It actually seems like there's not a whole lot of values that are super far away from the mean in terms of stds. If there were, we'd think about how to replace them or remove records (although removing would be a bad choice because we only have 149 records to start with)\n",
    "\n",
    "Let's find out the relationship between the labels and each of the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_grouped = merged.groupby(\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_grouped.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_csv(\"cleaned_testData1.csv\", header=False, index=False)\n",
    "labels.to_csv(\"cleaned_trainLabel1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
