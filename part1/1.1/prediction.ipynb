{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to restore variable 'e', ignoring (use %store -d to forget!)\n",
      "The error was: <class 'KeyError'>\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%store -r\n",
    "\n",
    "from time import time\n",
    "\n",
    "from math import sqrt, floor\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use = \"default\"\n",
    "\n",
    "import pandas as pd\n",
    "pd.options.display.float_format = '{:.3f}'.format\n",
    "\n",
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "from rfc import RandomForest\n",
    "from Experiment import Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"cleaned_testData1.csv\")\n",
    "labels = pd.read_csv(\"cleaned_trainLabel1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.drop(train.columns[0], axis=1)\n",
    "labels = labels.drop(labels.columns[0], axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f0</th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "      <th>f6</th>\n",
       "      <th>f7</th>\n",
       "      <th>f8</th>\n",
       "      <th>f9</th>\n",
       "      <th>...</th>\n",
       "      <th>f151</th>\n",
       "      <th>f152</th>\n",
       "      <th>f153</th>\n",
       "      <th>f154</th>\n",
       "      <th>f155</th>\n",
       "      <th>f156</th>\n",
       "      <th>f157</th>\n",
       "      <th>f158</th>\n",
       "      <th>f159</th>\n",
       "      <th>f160</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>149.000</td>\n",
       "      <td>149.000</td>\n",
       "      <td>149.000</td>\n",
       "      <td>149.000</td>\n",
       "      <td>149.000</td>\n",
       "      <td>149.000</td>\n",
       "      <td>149.000</td>\n",
       "      <td>149.000</td>\n",
       "      <td>149.000</td>\n",
       "      <td>149.000</td>\n",
       "      <td>...</td>\n",
       "      <td>149.000</td>\n",
       "      <td>149.000</td>\n",
       "      <td>149.000</td>\n",
       "      <td>149.000</td>\n",
       "      <td>149.000</td>\n",
       "      <td>149.000</td>\n",
       "      <td>149.000</td>\n",
       "      <td>149.000</td>\n",
       "      <td>149.000</td>\n",
       "      <td>149.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.513</td>\n",
       "      <td>1.204</td>\n",
       "      <td>1.262</td>\n",
       "      <td>1.888</td>\n",
       "      <td>1.390</td>\n",
       "      <td>1.668</td>\n",
       "      <td>1.305</td>\n",
       "      <td>1.743</td>\n",
       "      <td>2.567</td>\n",
       "      <td>1.913</td>\n",
       "      <td>...</td>\n",
       "      <td>1.562</td>\n",
       "      <td>2.009</td>\n",
       "      <td>1.751</td>\n",
       "      <td>2.210</td>\n",
       "      <td>2.285</td>\n",
       "      <td>2.707</td>\n",
       "      <td>2.787</td>\n",
       "      <td>1.225</td>\n",
       "      <td>1.920</td>\n",
       "      <td>2.801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.410</td>\n",
       "      <td>0.404</td>\n",
       "      <td>0.437</td>\n",
       "      <td>0.342</td>\n",
       "      <td>0.460</td>\n",
       "      <td>0.560</td>\n",
       "      <td>0.370</td>\n",
       "      <td>0.333</td>\n",
       "      <td>0.119</td>\n",
       "      <td>0.209</td>\n",
       "      <td>...</td>\n",
       "      <td>0.446</td>\n",
       "      <td>0.163</td>\n",
       "      <td>0.298</td>\n",
       "      <td>0.124</td>\n",
       "      <td>0.199</td>\n",
       "      <td>0.142</td>\n",
       "      <td>0.171</td>\n",
       "      <td>0.369</td>\n",
       "      <td>0.294</td>\n",
       "      <td>0.134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.257</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>2.349</td>\n",
       "      <td>1.343</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.651</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.894</td>\n",
       "      <td>1.829</td>\n",
       "      <td>2.389</td>\n",
       "      <td>2.391</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.483</td>\n",
       "      <td>2.493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.231</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.676</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.315</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.586</td>\n",
       "      <td>2.493</td>\n",
       "      <td>1.793</td>\n",
       "      <td>...</td>\n",
       "      <td>1.031</td>\n",
       "      <td>1.897</td>\n",
       "      <td>1.583</td>\n",
       "      <td>2.114</td>\n",
       "      <td>2.154</td>\n",
       "      <td>2.599</td>\n",
       "      <td>2.678</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.758</td>\n",
       "      <td>2.727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.475</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.839</td>\n",
       "      <td>1.252</td>\n",
       "      <td>1.652</td>\n",
       "      <td>1.198</td>\n",
       "      <td>1.727</td>\n",
       "      <td>2.549</td>\n",
       "      <td>1.896</td>\n",
       "      <td>...</td>\n",
       "      <td>1.597</td>\n",
       "      <td>2.005</td>\n",
       "      <td>1.723</td>\n",
       "      <td>2.197</td>\n",
       "      <td>2.262</td>\n",
       "      <td>2.707</td>\n",
       "      <td>2.785</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.863</td>\n",
       "      <td>2.780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.659</td>\n",
       "      <td>1.204</td>\n",
       "      <td>1.361</td>\n",
       "      <td>2.026</td>\n",
       "      <td>1.570</td>\n",
       "      <td>1.842</td>\n",
       "      <td>1.455</td>\n",
       "      <td>1.853</td>\n",
       "      <td>2.609</td>\n",
       "      <td>1.996</td>\n",
       "      <td>...</td>\n",
       "      <td>1.872</td>\n",
       "      <td>2.103</td>\n",
       "      <td>1.866</td>\n",
       "      <td>2.283</td>\n",
       "      <td>2.386</td>\n",
       "      <td>2.798</td>\n",
       "      <td>2.897</td>\n",
       "      <td>1.327</td>\n",
       "      <td>1.955</td>\n",
       "      <td>2.847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3.027</td>\n",
       "      <td>2.713</td>\n",
       "      <td>2.862</td>\n",
       "      <td>2.984</td>\n",
       "      <td>2.754</td>\n",
       "      <td>3.507</td>\n",
       "      <td>2.306</td>\n",
       "      <td>2.760</td>\n",
       "      <td>2.986</td>\n",
       "      <td>2.573</td>\n",
       "      <td>...</td>\n",
       "      <td>2.658</td>\n",
       "      <td>2.861</td>\n",
       "      <td>2.856</td>\n",
       "      <td>2.579</td>\n",
       "      <td>2.945</td>\n",
       "      <td>3.285</td>\n",
       "      <td>3.274</td>\n",
       "      <td>2.728</td>\n",
       "      <td>3.050</td>\n",
       "      <td>3.263</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 161 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           f0      f1      f2      f3      f4      f5      f6      f7      f8  \\\n",
       "count 149.000 149.000 149.000 149.000 149.000 149.000 149.000 149.000 149.000   \n",
       "mean    1.513   1.204   1.262   1.888   1.390   1.668   1.305   1.743   2.567   \n",
       "std     0.410   0.404   0.437   0.342   0.460   0.560   0.370   0.333   0.119   \n",
       "min     1.000   1.000   1.000   1.257   1.000   1.000   1.000   1.000   2.349   \n",
       "25%     1.231   1.000   1.000   1.676   1.000   1.315   1.000   1.586   2.493   \n",
       "50%     1.475   1.000   1.000   1.839   1.252   1.652   1.198   1.727   2.549   \n",
       "75%     1.659   1.204   1.361   2.026   1.570   1.842   1.455   1.853   2.609   \n",
       "max     3.027   2.713   2.862   2.984   2.754   3.507   2.306   2.760   2.986   \n",
       "\n",
       "           f9   ...      f151    f152    f153    f154    f155    f156    f157  \\\n",
       "count 149.000   ...   149.000 149.000 149.000 149.000 149.000 149.000 149.000   \n",
       "mean    1.913   ...     1.562   2.009   1.751   2.210   2.285   2.707   2.787   \n",
       "std     0.209   ...     0.446   0.163   0.298   0.124   0.199   0.142   0.171   \n",
       "min     1.343   ...     1.000   1.651   1.000   1.894   1.829   2.389   2.391   \n",
       "25%     1.793   ...     1.031   1.897   1.583   2.114   2.154   2.599   2.678   \n",
       "50%     1.896   ...     1.597   2.005   1.723   2.197   2.262   2.707   2.785   \n",
       "75%     1.996   ...     1.872   2.103   1.866   2.283   2.386   2.798   2.897   \n",
       "max     2.573   ...     2.658   2.861   2.856   2.579   2.945   3.285   3.274   \n",
       "\n",
       "         f158    f159    f160  \n",
       "count 149.000 149.000 149.000  \n",
       "mean    1.225   1.920   2.801  \n",
       "std     0.369   0.294   0.134  \n",
       "min     1.000   1.483   2.493  \n",
       "25%     1.000   1.758   2.727  \n",
       "50%     1.000   1.863   2.780  \n",
       "75%     1.327   1.955   2.847  \n",
       "max     2.728   3.050   3.263  \n",
       "\n",
       "[8 rows x 161 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(df, labels):\n",
    "    return labels.merge(df, left_index=True,right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mprint(*args):\n",
    "    for arg in args:\n",
    "        print(arg)\n",
    "        print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"8\" halign=\"left\">f0</th>\n",
       "      <th colspan=\"2\" halign=\"left\">f1</th>\n",
       "      <th>...</th>\n",
       "      <th colspan=\"2\" halign=\"left\">f98</th>\n",
       "      <th colspan=\"8\" halign=\"left\">f99</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>...</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>107.000</td>\n",
       "      <td>1.444</td>\n",
       "      <td>0.285</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.247</td>\n",
       "      <td>1.465</td>\n",
       "      <td>1.614</td>\n",
       "      <td>2.378</td>\n",
       "      <td>107.000</td>\n",
       "      <td>1.099</td>\n",
       "      <td>...</td>\n",
       "      <td>2.425</td>\n",
       "      <td>2.565</td>\n",
       "      <td>107.000</td>\n",
       "      <td>2.622</td>\n",
       "      <td>0.102</td>\n",
       "      <td>2.388</td>\n",
       "      <td>2.549</td>\n",
       "      <td>2.625</td>\n",
       "      <td>2.693</td>\n",
       "      <td>2.883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14.000</td>\n",
       "      <td>1.190</td>\n",
       "      <td>0.235</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.051</td>\n",
       "      <td>1.370</td>\n",
       "      <td>1.630</td>\n",
       "      <td>14.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.487</td>\n",
       "      <td>2.521</td>\n",
       "      <td>14.000</td>\n",
       "      <td>2.586</td>\n",
       "      <td>0.087</td>\n",
       "      <td>2.431</td>\n",
       "      <td>2.530</td>\n",
       "      <td>2.581</td>\n",
       "      <td>2.615</td>\n",
       "      <td>2.796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.000</td>\n",
       "      <td>1.320</td>\n",
       "      <td>0.245</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.175</td>\n",
       "      <td>1.294</td>\n",
       "      <td>1.438</td>\n",
       "      <td>1.837</td>\n",
       "      <td>11.000</td>\n",
       "      <td>1.019</td>\n",
       "      <td>...</td>\n",
       "      <td>2.458</td>\n",
       "      <td>2.541</td>\n",
       "      <td>11.000</td>\n",
       "      <td>2.785</td>\n",
       "      <td>0.057</td>\n",
       "      <td>2.661</td>\n",
       "      <td>2.762</td>\n",
       "      <td>2.788</td>\n",
       "      <td>2.804</td>\n",
       "      <td>2.871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14.000</td>\n",
       "      <td>2.352</td>\n",
       "      <td>0.248</td>\n",
       "      <td>2.081</td>\n",
       "      <td>2.167</td>\n",
       "      <td>2.300</td>\n",
       "      <td>2.451</td>\n",
       "      <td>3.027</td>\n",
       "      <td>14.000</td>\n",
       "      <td>2.277</td>\n",
       "      <td>...</td>\n",
       "      <td>2.657</td>\n",
       "      <td>2.799</td>\n",
       "      <td>14.000</td>\n",
       "      <td>2.869</td>\n",
       "      <td>0.146</td>\n",
       "      <td>2.655</td>\n",
       "      <td>2.766</td>\n",
       "      <td>2.864</td>\n",
       "      <td>2.936</td>\n",
       "      <td>3.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3.000</td>\n",
       "      <td>2.250</td>\n",
       "      <td>0.327</td>\n",
       "      <td>1.883</td>\n",
       "      <td>2.120</td>\n",
       "      <td>2.357</td>\n",
       "      <td>2.434</td>\n",
       "      <td>2.512</td>\n",
       "      <td>3.000</td>\n",
       "      <td>1.590</td>\n",
       "      <td>...</td>\n",
       "      <td>2.542</td>\n",
       "      <td>2.576</td>\n",
       "      <td>3.000</td>\n",
       "      <td>2.713</td>\n",
       "      <td>0.091</td>\n",
       "      <td>2.629</td>\n",
       "      <td>2.664</td>\n",
       "      <td>2.699</td>\n",
       "      <td>2.755</td>\n",
       "      <td>2.810</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1288 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           f0                                                f1        ...   \\\n",
       "        count  mean   std   min   25%   50%   75%   max   count  mean  ...    \n",
       "label                                                                  ...    \n",
       "1     107.000 1.444 0.285 1.000 1.247 1.465 1.614 2.378 107.000 1.099  ...    \n",
       "2      14.000 1.190 0.235 1.000 1.000 1.051 1.370 1.630  14.000 1.000  ...    \n",
       "3      11.000 1.320 0.245 1.000 1.175 1.294 1.438 1.837  11.000 1.019  ...    \n",
       "4      14.000 2.352 0.248 2.081 2.167 2.300 2.451 3.027  14.000 2.277  ...    \n",
       "5       3.000 2.250 0.327 1.883 2.120 2.357 2.434 2.512   3.000 1.590  ...    \n",
       "\n",
       "        f98           f99                                            \n",
       "        75%   max   count  mean   std   min   25%   50%   75%   max  \n",
       "label                                                                \n",
       "1     2.425 2.565 107.000 2.622 0.102 2.388 2.549 2.625 2.693 2.883  \n",
       "2     2.487 2.521  14.000 2.586 0.087 2.431 2.530 2.581 2.615 2.796  \n",
       "3     2.458 2.541  11.000 2.785 0.057 2.661 2.762 2.788 2.804 2.871  \n",
       "4     2.657 2.799  14.000 2.869 0.146 2.655 2.766 2.864 2.936 3.125  \n",
       "5     2.542 2.576   3.000 2.713 0.091 2.629 2.664 2.699 2.755 2.810  \n",
       "\n",
       "[5 rows x 1288 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge(train, labels).groupby(\"label\").describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---  \n",
    "\n",
    "# Model Building\n",
    "\n",
    "## Which algorithm to use?\n",
    "We'll use a **random forest classifier** (rfc) with bootstrapping and feature bagging optimizations because:\n",
    "- ease of implementation\n",
    "- rfcs handle multi-class predictions well without more additional effort\n",
    "- works well with high dimensional data\n",
    "- we'll choose use random forest as opposed to boosted trees since we have highly dimensional data\n",
    "- with a reasonably high probability, can be used with the other datasets for this project since the algorithm is very robust\n",
    "\n",
    "## The Algorithm\n",
    "We'll use the CART algorithm for splitting since we have continuous data.  \n",
    "  \n",
    "[Full example](https://machinelearningmastery.com/classification-and-regression-trees-for-machine-learning/)  \n",
    "  \n",
    "Steps:\n",
    "1. Initialize Tree\n",
    "2. For each column, calc best split across all rows based using gini impurity score - [exmplanation](https://en.wikipedia.org/wiki/Decision_tree_learning#Gini_impurity) | [exmaple](https://www.researchgate.net/post/How_to_compute_impurity_using_Gini_Index) | [useful blog](http://dni-institute.in/blogs/cart-algorithm-for-decision-tree/)\n",
    "3. Split the dataset based on the split condition with the highest gini score and add both sets as leaves on a tree node. The node represents a decision point, that being the condition with the highest gini score.\n",
    "3. Repeat 2 & 3 until an arbitrary minimum number of rows are left\n",
    "4. Prune tree\n",
    "\n",
    "ideas:\n",
    "- instead of using the raw values, categorize the numbers as # of stds away from mean\n",
    "- > Alternatively, the random forest can apply weight concept for considering the impact of result from any decision tree. Tree with high error rate are given low weight value and vise versa. This would increase the decision impact of trees with low error rate - [medium post](https://medium.com/machine-learning-101/chapter-5-random-forest-classifier-56dc7425c3e1)\n",
    "- [parameters to  tune](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)\n",
    "https://www.analyticsvidhya.com/blog/2016/04/complete-tutorial-tree-based-modeling-scratch-in-python/\n",
    "- https://www.analyticsvidhya.com/blog/2015/06/tuning-random-forest-model/\n",
    "- https://stats.stackexchange.com/questions/260460/optimization-of-a-random-forest-model\n",
    "- https://followthedata.wordpress.com/2012/06/02/practical-advice-for-machine-learning-bias-variance/\n",
    "- https://towardsdatascience.com/beyond-accuracy-precision-and-recall-3da06bea9f6c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---  \n",
    "# Model Training & Tuning\n",
    "## Context\n",
    "Now that we have our classifier, let's think about how we're going to train the model. \n",
    "\n",
    "We'll also measure performance through [precision](https://en.wikipedia.org/wiki/Precision_and_recall) & [recall](https://towardsdatascience.com/beyond-accuracy-precision-and-recall-3da06bea9f6c) - it tells us, for each class, how well the model identifies all cases of that class (recall) and how well it can correctly classify those cases (precision). From wikipedia:\n",
    "> Suppose a computer program for recognizing dogs in photographs identifies eight dogs in a picture containing 12 dogs and some cats. Of the eight dogs identified, five actually are dogs (true positives), while the rest are cats (false positives). The program's precision is 5/8 while its recall is 5/12.\n",
    "\n",
    "![precision & recall formulas](https://cdn-images-1.medium.com/max/2000/1*6NkN_LINs2erxgVJ9rkpUA.png)\n",
    "We can use the [f1 score](https://en.wikipedia.org/wiki/F1_score) to maximize precision and recall when testing different models.  \n",
    "![f1 score formula](https://cdn-images-1.medium.com/max/1600/1*UJxVqLnbSj42eRhasKeLOA.png)\n",
    "\n",
    "Recall and precision seem to be very related to bias and variance of the model, so we can maximize the f1 score by tuning the model to affect these.\n",
    "#### Minimizing bias\n",
    "- use new/different features\n",
    "- increase the size of the trees (increases variance)\n",
    "- increase the number of trees in the forest\n",
    "\n",
    "#### Minimizing variance\n",
    "- decrease the number of features\n",
    "    + probably want to aim to features that are correlated and/or collapse the overall number of features through PCA\n",
    "- use more data for each tree  \n",
    "\n",
    "  \n",
    "Beware: too much completixy is bad & not enough complexity is also bad  \n",
    "![bias variance tradeoff](http://scott.fortmann-roe.com/docs/docs/BiasVariance/biasvariance.png)  \n",
    "  \n",
    "\n",
    "#### Stability\n",
    "We need to make sure to train the classifier on as many data points as possible while also leaving enough to test to reliably tell how well the classifier actually performs. We'll use [k-fold cross validation](https://www.analyticsvidhya.com/blog/2015/11/improve-model-performance-cross-validation-in-python-r/):  \n",
    "  \n",
    "> 1. Randomly split your entire dataset into k ”folds”.\n",
    "2. For each k folds in your dataset, build your model on k – 1 folds of the data set. Then, test the model to check the effectiveness for kth fold.\n",
    "3. Record the error you see on each of the predictions.\n",
    "4. Repeat this until each of the k folds has served as the test set.\n",
    "\n",
    "## Procedure\n",
    "1. Record and save an input configuration for the random forest\n",
    "1. Separate data into k folds\n",
    "2. For each fold *k*: \n",
    "    1. train the classifier on k-1 folds\n",
    "    2. predict the k-th fold\n",
    "    3. measure the: accuracy, [logarithmic](http://wiki.fast.ai/index.php/Log_Loss) [loss](https://towardsdatascience.com/metrics-to-evaluate-your-machine-learning-algorithm-f10ba6e38234#f217), recall, precision, and f1-score\n",
    "3. Record the performance measures & associate it with the input configuration\n",
    "3. Evaluate the overall performance difference across all configurations\n",
    "4. Change variables from the input configuration that optimizes model perfomance & repeat steps 1-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_config_names = [\"num_trees\", \"num_features\", \"num_sample_rows\", \"max_tree_depth\", \"min_split_samples\", \"bias_class\", \"bias_amount\"]\n",
    "init_config_values = [   10,          None,           None,              20,                5,                 None,            0] # default settings = initial settings\n",
    "if not e:\n",
    "    e = Experiment(train, labels, RandomForest, init_config_names, init_config_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(e.experiments) == 0:\n",
    "    t1 = e.run_trial()\n",
    "    %store e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "perf, train_test = e.trial_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trial 1 Notes:\n",
    "\n",
    "These settings were default. Since we have 5 classes, the range of log loss is between 0 and 1.6, meaning our log loss is pretty bad and that we're very likely to misclassify. The accuracy seems good, but that's only because class 1 has a higher chance to appear in general as it appears ~70% of the time. As we see in iteration 7, accuracy and precision are _.533_ and _.284_  respectively. That iteration probably had more diverse labels than usual, but the model only got it right $\\frac{1}{4}^{th}%$ of the time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_7_train, t_7_test, t_7_predict  = e.experiment_data[6]\n",
    "\n",
    "t_7_train_g = merge(t_7_train[0], labels).groupby(\"label\")\n",
    "t_7_test_g = merge(t_7_test, labels).groupby(\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x11484a9e8>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x114857208>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "e.experiments.sort_values([\"recall\", \"precision\"]).plot(\"recall\", \"precision\", \"line\", figsize=(5,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very odd that precision and recall move in step.. Not sure what that means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "log_loss         1.609\n",
       "class_accuracy   0.739\n",
       "precision        0.564\n",
       "recall           0.739\n",
       "f1-score         0.635\n",
       "support            nan\n",
       "dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e.experiments[e.performance_measures()].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1   0.282\n",
       "2     nan\n",
       "3     nan\n",
       "4     nan\n",
       "5     nan\n",
       "dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expected_means = merge(train, labels).groupby(\"label\").count().apply(lambda x: x / train.shape[0]).T.mean()\n",
    "\n",
    "# average bias\n",
    "pd.DataFrame([i[2].value_counts(normalize=True) - expected_means for i in e.experiment_data]).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, it seems like the classifier is stuck on the imbalance of classes in the dataset. We have close to 0 variance but lots of bias. We can count the `nan`s above as $0 - class$.  \n",
    "\n",
    "To combat this, we'll add more & deeper trees in hopes that the model will pick up on more variance, train some trees only on data that have the imbalanced classes, and use less data to train the model with in order to reduce the extent to which a single class and dominate the others.  \n",
    "  \n",
    "We can also increase the number of samples that we give a tree to something that will mirror the overall statistical properties of the original set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "1   0.683\n",
       "2   0.100\n",
       "3   0.083\n",
       "4   0.117\n",
       "5   0.017\n",
       "dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run this a few times to minimize s_rows and statistical difference in label means\n",
    "s_rows = 60\n",
    "row_indices = np.random.choice(train.shape[0], size=s_rows, replace=False)\n",
    "merge(train.iloc[row_indices], labels).groupby(\"label\").count().apply(lambda x: x / s_rows).T.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment.Experiment at 0x1108ede80>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e.tweak(\"num_trees\", 20).tweak(\"min_split_samples\", 1).tweak(\"num_sample_rows\", s_rows).tweak(\"num_features\", 20).tweak(\"bias_class\", 1).tweak(\"bias_amount\", .7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running trail #11\n",
      "------------------------------------\n",
      " \n",
      "*************************\n",
      " \n",
      "Running fold 1 of 10\n",
      " \n",
      "*************************\n",
      " \n",
      "fold 1 took 87.1112699508667s\n",
      " \n",
      "*************************\n",
      " \n",
      "Running fold 2 of 10\n",
      " \n",
      "*************************\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/python3/3.5.0/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 2 took 54.9058620929718s\n",
      " \n",
      "*************************\n",
      " \n",
      "Running fold 3 of 10\n",
      " \n",
      "*************************\n",
      " \n",
      "fold 3 took 95.08624792098999s\n",
      " \n",
      "*************************\n",
      " \n",
      "Running fold 4 of 10\n",
      " \n",
      "*************************\n",
      " \n",
      "fold 4 took 77.98707008361816s\n",
      " \n",
      "*************************\n",
      " \n",
      "Running fold 5 of 10\n",
      " \n",
      "*************************\n",
      " \n",
      "fold 5 took 66.67003893852234s\n",
      " \n",
      "*************************\n",
      " \n",
      "Running fold 6 of 10\n",
      " \n",
      "*************************\n",
      " \n",
      "fold 6 took 66.7773768901825s\n",
      " \n",
      "*************************\n",
      " \n",
      "Running fold 7 of 10\n",
      " \n",
      "*************************\n",
      " \n",
      "fold 7 took 53.39123201370239s\n",
      " \n",
      "*************************\n",
      " \n",
      "Running fold 8 of 10\n",
      " \n",
      "*************************\n",
      " \n",
      "fold 8 took 109.56289982795715s\n",
      " \n",
      "*************************\n",
      " \n",
      "Running fold 9 of 10\n",
      " \n",
      "*************************\n",
      " \n",
      "fold 9 took 67.45301985740662s\n",
      " \n",
      "*************************\n",
      " \n",
      "Running fold 10 of 10\n",
      " \n",
      "*************************\n",
      " \n",
      "fold 10 took 66.93995213508606s\n",
      " \n",
      "Stored 'e' (Experiment)\n"
     ]
    }
   ],
   "source": [
    "if e.experiments[e.experiments[\"trial_num\"] > 1].shape[0] < 2:\n",
    "    e.run_trial()\n",
    "    %store e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2_results, t2_train_test =  e.trial_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_trees</th>\n",
       "      <th>num_features</th>\n",
       "      <th>num_sample_rows</th>\n",
       "      <th>max_tree_depth</th>\n",
       "      <th>min_split_samples</th>\n",
       "      <th>bias_class</th>\n",
       "      <th>bias_amount</th>\n",
       "      <th>log_loss</th>\n",
       "      <th>class_accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "      <th>trial_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>20.000</td>\n",
       "      <td>20.000</td>\n",
       "      <td>60.000</td>\n",
       "      <td>20.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.700</td>\n",
       "      <td>1.609</td>\n",
       "      <td>0.867</td>\n",
       "      <td>0.751</td>\n",
       "      <td>0.867</td>\n",
       "      <td>0.805</td>\n",
       "      <td>nan</td>\n",
       "      <td>2.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>20.000</td>\n",
       "      <td>20.000</td>\n",
       "      <td>60.000</td>\n",
       "      <td>20.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.700</td>\n",
       "      <td>1.609</td>\n",
       "      <td>0.733</td>\n",
       "      <td>0.538</td>\n",
       "      <td>0.733</td>\n",
       "      <td>0.621</td>\n",
       "      <td>nan</td>\n",
       "      <td>2.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>20.000</td>\n",
       "      <td>20.000</td>\n",
       "      <td>60.000</td>\n",
       "      <td>20.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.700</td>\n",
       "      <td>1.609</td>\n",
       "      <td>0.733</td>\n",
       "      <td>0.538</td>\n",
       "      <td>0.733</td>\n",
       "      <td>0.621</td>\n",
       "      <td>nan</td>\n",
       "      <td>2.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>20.000</td>\n",
       "      <td>20.000</td>\n",
       "      <td>60.000</td>\n",
       "      <td>20.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.700</td>\n",
       "      <td>1.609</td>\n",
       "      <td>0.667</td>\n",
       "      <td>0.444</td>\n",
       "      <td>0.667</td>\n",
       "      <td>0.533</td>\n",
       "      <td>nan</td>\n",
       "      <td>2.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>20.000</td>\n",
       "      <td>20.000</td>\n",
       "      <td>60.000</td>\n",
       "      <td>20.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.700</td>\n",
       "      <td>1.609</td>\n",
       "      <td>0.800</td>\n",
       "      <td>0.640</td>\n",
       "      <td>0.800</td>\n",
       "      <td>0.711</td>\n",
       "      <td>nan</td>\n",
       "      <td>2.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>20.000</td>\n",
       "      <td>20.000</td>\n",
       "      <td>60.000</td>\n",
       "      <td>20.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.700</td>\n",
       "      <td>1.609</td>\n",
       "      <td>0.667</td>\n",
       "      <td>0.444</td>\n",
       "      <td>0.667</td>\n",
       "      <td>0.533</td>\n",
       "      <td>nan</td>\n",
       "      <td>2.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>20.000</td>\n",
       "      <td>20.000</td>\n",
       "      <td>60.000</td>\n",
       "      <td>20.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.700</td>\n",
       "      <td>1.609</td>\n",
       "      <td>0.733</td>\n",
       "      <td>0.538</td>\n",
       "      <td>0.733</td>\n",
       "      <td>0.621</td>\n",
       "      <td>nan</td>\n",
       "      <td>2.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>20.000</td>\n",
       "      <td>20.000</td>\n",
       "      <td>60.000</td>\n",
       "      <td>20.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.700</td>\n",
       "      <td>1.609</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.360</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.450</td>\n",
       "      <td>nan</td>\n",
       "      <td>2.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>20.000</td>\n",
       "      <td>20.000</td>\n",
       "      <td>60.000</td>\n",
       "      <td>20.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.700</td>\n",
       "      <td>1.609</td>\n",
       "      <td>0.667</td>\n",
       "      <td>0.444</td>\n",
       "      <td>0.667</td>\n",
       "      <td>0.533</td>\n",
       "      <td>nan</td>\n",
       "      <td>2.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20.000</td>\n",
       "      <td>20.000</td>\n",
       "      <td>60.000</td>\n",
       "      <td>20.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.700</td>\n",
       "      <td>1.609</td>\n",
       "      <td>0.714</td>\n",
       "      <td>0.510</td>\n",
       "      <td>0.714</td>\n",
       "      <td>0.595</td>\n",
       "      <td>nan</td>\n",
       "      <td>2.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    num_trees  num_features  num_sample_rows  max_tree_depth  \\\n",
       "10     20.000        20.000           60.000          20.000   \n",
       "11     20.000        20.000           60.000          20.000   \n",
       "12     20.000        20.000           60.000          20.000   \n",
       "13     20.000        20.000           60.000          20.000   \n",
       "14     20.000        20.000           60.000          20.000   \n",
       "15     20.000        20.000           60.000          20.000   \n",
       "16     20.000        20.000           60.000          20.000   \n",
       "17     20.000        20.000           60.000          20.000   \n",
       "18     20.000        20.000           60.000          20.000   \n",
       "19     20.000        20.000           60.000          20.000   \n",
       "\n",
       "    min_split_samples  bias_class  bias_amount  log_loss  class_accuracy  \\\n",
       "10              1.000       1.000        0.700     1.609           0.867   \n",
       "11              1.000       1.000        0.700     1.609           0.733   \n",
       "12              1.000       1.000        0.700     1.609           0.733   \n",
       "13              1.000       1.000        0.700     1.609           0.667   \n",
       "14              1.000       1.000        0.700     1.609           0.800   \n",
       "15              1.000       1.000        0.700     1.609           0.667   \n",
       "16              1.000       1.000        0.700     1.609           0.733   \n",
       "17              1.000       1.000        0.700     1.609           0.600   \n",
       "18              1.000       1.000        0.700     1.609           0.667   \n",
       "19              1.000       1.000        0.700     1.609           0.714   \n",
       "\n",
       "    precision  recall  f1-score  support  trial_num  \n",
       "10      0.751   0.867     0.805      nan      2.000  \n",
       "11      0.538   0.733     0.621      nan      2.000  \n",
       "12      0.538   0.733     0.621      nan      2.000  \n",
       "13      0.444   0.667     0.533      nan      2.000  \n",
       "14      0.640   0.800     0.711      nan      2.000  \n",
       "15      0.444   0.667     0.533      nan      2.000  \n",
       "16      0.538   0.733     0.621      nan      2.000  \n",
       "17      0.360   0.600     0.450      nan      2.000  \n",
       "18      0.444   0.667     0.533      nan      2.000  \n",
       "19      0.510   0.714     0.595      nan      2.000  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# e.experiments\n",
    "e.experiment_data[9][2].to_csv(\"predictions.txt\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1148c67f0>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUYAAAFACAYAAADAjtXtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAIABJREFUeJzt3XeYlPW9/vH3x6Ws0quFtouAICBtdkEMahILMUfsCop0sJEYE3PUJL+Y4MmJMSdqYlBDL1IsiUrUxKhobJQdepMqyALC0suybPv8/tjBDCuyA+zuM7N7v65rL+Zps/cMePvMPPP9jrk7IiLyH2cEHUBEJN6oGEVEilExiogUo2IUESlGxSgiUoyKUUSkGBWjiEgxKkYRkWJUjCIixVQJOkBxDRs29JSUlKBjiEgFs2DBgp3u3iiWfeOuGFNSUgiHw0HHEJEKxsw2xbqvXkqLiBSjYhQRKUbFKCJSTNy9x3g8eXl5ZGZmkpOTE3SUhJWcnEzTpk2pWrVq0FFE4l5CFGNmZia1atUiJSUFMws6TsJxd3bt2kVmZiapqalBxxGJewnxUjonJ4cGDRqoFE+RmdGgQQOdcYvEKCGKEVApniY9fyKxS5hiFBEpLzEVo5n1NrPVZrbOzB4+zvanzGxx5GeNme2N2lYQtW1WaYavCHr27HnC7ddccw179+494T4iUrpKvPhiZknAaOBKIBPIMLNZ7r7y6D7u/kDU/j8AukTdxWF371x6keNXQUEBSUlJJ3XMp59+esLtb7311ulEEqmQZs7/gkvbNOK8umeWyf3HcsaYDqxz9w3ungvMBK47wf79gBmlES6ebNy4kbZt23LHHXfQrl07br75ZrKzs0lJSeGhhx6ia9euvPzyy6xfv57evXvTrVs3evXqxWeffQbA9u3bueGGG+jUqROdOnX6qhBr1qwJwLZt27j00kvp3LkzHTp04KOPPgKKhkju3LkTgCeffJIOHTrQoUMHnn766a9ytWvXjuHDh9O+fXuuuuoqDh8+XN5Pj0i5eSm8mYf/toxxH31eZr8jlo/rNAE2Ry1nAt2Pt6OZtQBSgdlRq5PNLAzkA4+7+2vHOW4EMAKgefPmJwzz67+vYOXW/THEjt2F59Xm0Wvbl7jf6tWrGT9+PJdccglDhgzh2WefBaBBgwYsXLgQgO9+97s8//zztG7dmnnz5nHvvfcye/ZsfvjDH3LZZZfx6quvUlBQwMGDB4+57+nTp3P11Vfz85//nIKCArKzs4/ZvmDBAiZOnMi8efNwd7p3785ll11GvXr1WLt2LTNmzGDs2LHceuut/PWvf6V///6l9OyIxI/3Vm3nkb8to1frhjz8vbZl9ntK+3OMfYFX3L0gal0Ld99iZi2B2Wa2zN3XRx/k7mOAMQChUChuv+i6WbNmXHLJJQD079+fP/3pTwDcdtttABw8eJBPP/2UW2655atjjhw5AsDs2bOZMmUKAElJSdSpU+eY+05LS2PIkCHk5eVx/fXX07nzse8+fPzxx9xwww3UqFEDgBtvvJGPPvqIPn36kJqa+tX+3bp1Y+PGjaX8yEWCt2DTHu6bvpD259Xmuf7dqFal7K4dx1KMW4BmUctNI+uOpy9wX/QKd98S+XODmX1A0fuP679+aGxiObMrK8U/8nJ0+WhZFRYWUrduXRYvXnzS933ppZfy4Ycf8uabbzJo0CB+/OMfM2DAgJiOrV69+le3k5KS9FJaKpx1Ow4wdHIG59ROZsKgNGpWL9uxKbFUbgbQ2sxSzawaReX3tavLZtYWqAfMiVpXz8yqR243BC4BVhY/NlF88cUXzJlT9PCmT5/Ot771rWO2165dm9TUVF5++WWgaMTJkiVLgKKX2M899xxQdJFm3759xxy7adMmzj77bIYPH86wYcO+eml+VK9evXjttdfIzs7m0KFDvPrqq/Tq1atMHqdIPNm27zADxs+nyhlnMGVIdxrWrF7yQaepxGJ093xgJPA2sAp4yd1XmNkoM+sTtWtfYKa7R78UbgeEzWwJ8D5F7zEmbDFecMEFjB49mnbt2rFnzx7uueeer+0zbdo0xo8fT6dOnWjfvj2vv/46AH/84x95//336dixI926dWPlymOfhg8++IBOnTrRpUsXXnzxRe6///5jtnft2pVBgwaRnp5O9+7dGTZsGF26dEGkItuXncfACfPZn5PPpMFpNG9wVrn8Xju2x4IXCoW8+ES1q1atol27dgElKrJx40b+67/+i+XLlwea43TEw/MoEqucvALuHD+PJZv3MWlwGj1bNTyt+zOzBe4eimXfhJhEQkQql/yCQn4wYxHhTXv4c7+up12KJ0tDAmOUkpKS0GeLIonC3fl/ry/nnZXb+dW17fn+ReeWe4aEKcZ4e8mfaPT8SaJ46t21zJi/mfu+fT4De6YEkiEhijE5OZldu3bpP+5TdHQ+xuTk5KCjiJzQ1Lmb+NN7a7k11JQHr7ogsBwJ8R5j06ZNyczMJCsrK+goCevoDN4i8eofy7bxy9eXc0W7xvzvDR0DnSovIYqxatWqmnlapAKbu2EX989cTJdmdXmmX1eqJAX7YjYhXkqLSMW1cut+hk8O07zBWUwYlMaZ1U5uhqqyoGIUkcBs3p3NwInzqZlchSlD0ql7VrWgIwEqRhEJyK6DRxg4YT5H8gqYPCS9zOZWPBUJ8R6jiFQsh47kM2RymC17DzNtWHfanF0r6EjH0BmjiJSrvIJC7pm2kGWZe/nz7V0JpdQPOtLX6IxRRMpNYaHz0CtL+XBNFo/f2JErLzw76EjHpTNGESk3v/vnZ/xt0RZ+cmUb+qafeLb+IKkYRaRcjPtoA3/5cAMDLm7ByO+0CjrOCakYRaTMvbZoC//z5iqu6XgOj17bPtBRLbFQMYpImfpwTRYPvryEHi3r8+StnUk6I75LEVSMIlKGlmbu5e4XFtD67FqMGRAiuWrwo1pioWIUkTLx+c5DDJ6YQf0a1Zg8OI3ayVWDjhQzFaOIlLodB3IYMGEeDkwZkk7j2ok15Z2KUURK1YGcPAZNyGDngVwmDEqjZaOaQUc6aSpGESk1R/ILuGvqAtZsP8Bz/bvSuVndoCOdEo18EZFSUVDo/PjFJXy6fhdP3daJyy9oHHSkUxbTGaOZ9Taz1Wa2zswePs72p8xsceRnjZntjdo20MzWRn4GlmZ4EYkP7s6ov6/gzWXb+Nk1bbmhS2LPFl/iGaOZJQGjgSuBTCDDzGa5+1ffGO/uD0Tt/wOgS+R2feBRIAQ4sCBy7J5SfRQiEqhnP1jP5DmbGPatVEZcen7QcU5bLGeM6cA6d9/g7rnATOC6E+zfD5gRuX018I67746U4TtA79MJLCLx5aWMzfz+7dVc3/k8fnZNu6DjlIpYirEJsDlqOTOy7mvMrAWQCsw+2WNFJPG8u3I7j7y6jF6tG/LEzZ04IwFGtcSitK9K9wVecfeCkznIzEaYWdjMwvomQJHEsGDTbu6bvpD259Xm+f7dqFal4nzIJZZHsgVoFrXcNLLuePryn5fRMR/r7mPcPeTuoUaNGsUQSUSCtHb7AYZMCnNunWQmDEqjRvWK9QGXWIoxA2htZqlmVo2i8ptVfCczawvUA+ZErX4buMrM6plZPeCqyDoRSVDb9h1mwIT5VKtyBlOHdqdhzepBRyp1Jda8u+eb2UiKCi0JmODuK8xsFBB296Ml2ReY6e4edexuM3uMonIFGOXuu0v3IYhIedmbncuA8fM5kJPPi3f1oFn9s4KOVCYsqsfiQigU8nA4HHQMESkmJ6+A/uPmsTRzH5OGpNHz/IZBRzopZrbA3UOx7Fux3hgQkTKRX1DIyOmLWPDFHv7cr2vCleLJqjiXkUSkTLg7v3htOe+u2s6vrm3P9y86N+hIZU7FKCIn9OQ7a5iZsZmR327FwJ4pQccpFypGEflGU+ds5JnZ67gt1IyfXNUm6DjlRsUoIsf11rJt/HLWCq5o15jf3NAh7r/AqjSpGEXka+as38WPZi6ma/N6PNOvK1WSKldVVK5HKyIlWrl1PyOmhGnR4CzGDwxxZrXE+AKr0qRiFJGvbN6dzcCJ86mZXIXJQ9Kpe1a1oCMFQsUoIgDsOniEARPmcySvgMlD0jmv7plBRwqMPuAtIhw6ks+QSRls3XuYacO60+bsWkFHCpTOGEUqudz8Qu6ZtpBlW/bx59u7EkqpH3SkwOmMUaQSKyx0HvrrUj5ck8XjN3bkygvPDjpSXNAZo0gl9vg/P+PVRVv4yZVt6JvePOg4cUPFKFJJjf1wA2M+3MCAi1sw8jutgo4TV1SMIpXQq4sy+c1bq7im4zk8em37SjWqJRYqRpFK5t9rsvjpy0vp0bI+T97amaQK8gVWpUnFKFKJLNm8l3teWEDrs2sxZkCI5KqVb1RLLFSMIpXEhqyDDJ6UQf0a1Zg8OI3ayVWDjhS3VIwilcCO/TkMmDAfgClD0mlcOzngRPFNxShSwe3PyWPgxAx2H8pl4qA0WjaqGXSkuKdiFKnAjuQXMGJKmLXbD/Bc/250alY36EgJQSNfRCqogkLngRcXM3fDbp66rROXtWkUdKSEoTNGkQrI3fn131fw1rIv+fk17bihS9OgIyWUmIrRzHqb2WozW2dmD3/DPrea2UozW2Fm06PWF5jZ4sjPrNIKLiLfbPT765gyZxPDe6Uy/NKWQcdJOCW+lDazJGA0cCWQCWSY2Sx3Xxm1T2vgEeASd99jZo2j7uKwu3cu5dwi8g1mzv+C//vXGq7vfB6PfK9d0HESUixnjOnAOnff4O65wEzgumL7DAdGu/seAHffUboxRSQW76zczs9eXUav1g154uZOnKFRLacklmJsAmyOWs6MrIvWBmhjZp+Y2Vwz6x21LdnMwpH11x/vF5jZiMg+4aysrJN6ACJSJLxxNyOnL6Rjkzo8378b1aroEsKpKq2r0lWA1sDlQFPgQzPr6O57gRbuvsXMWgKzzWyZu6+PPtjdxwBjAEKhkJdSJpFKY832AwydHOa8umcyYVAaNarrAyenI5b/pWwBmkUtN42si5YJzHL3PHf/HFhDUVHi7lsif24APgC6nGZmEYmyde9hBk6YT7UqZzBlSDoNalYPOlLCi6UYM4DWZpZqZtWAvkDxq8uvUXS2iJk1pOil9QYzq2dm1aPWXwKsRERKxd7sXAZMmM+BnHwmDU6jWf2zgo5UIZR4vu3u+WY2EngbSAImuPsKMxsFhN19VmTbVWa2EigAfuruu8ysJ/AXMyukqIQfj76aLSKn7nBuAUMnh/liVzaThqTR/rw6QUeqMMw9vt7SC4VCHg6Hg44hEtfyCwq5+4UFvPfZDkbf3pVrOp4bdKS4Z2YL3D0Uy766bCWSYAoKnZ++spR3V+3g133aqxTLgIpRJIEc/Va/Vxdt4cGr2jDg4pSgI1VIKkaRBFFY6Pzs1WW8siCTH13RmpHfaR10pApLxSiSANyd//f6cmZmbGbkt1tx/3dVimVJxSgS59ydX81awbR5X3DXZS35yVVt9K1+ZUzFKBLH3J3H3ljF5DmbGPatVB7u3ValWA5UjCJxyt15/B+fMeGTzxnUM4Wff7+dSrGcqBhF4pC783//Ws1fPtxA/x7NefTaC1WK5UjFKBKHnn53LaPfX0+/9GaM6tNBpVjOVIwiceaZ99byx/fWcku3pvzm+o6aUzEAKkaROPLsB+v4wztruLFLEx6/6SKVYkBUjCJxYuyHG3jin6vp0+k8fn9LJ5JUioFRMYrEgQkff85v3lrF9zuey5O3qhSDpmIUCdiUORsZ9cZKrm5/Nk/37UyVJP1nGTT9DYgEaPq8L/jl6yu4ol1jnunXlaoqxbigvwWRgLyUsZmfvbqMb1/QiNF3dNWXV8UR/U2IBOCvCzJ56G9L6dW6Ic/170b1KklBR5IoKkaRcvb64i08+MoSep7fgLEDQiRXVSnGGxWjSDl6Y+lWHnhxMekp9Rk3IE2lGKdUjCLl5B/LtnH/zMV0a1GPCYPSOLOaSjFeqRhFysG/VnzJD2YsolPTOkwcnE6N6iV+QacESMUoUsZmf7ad+6YvpH2TOkwakk5NlWLcUzGKlKF/r8ni7qkLaXtObaYMSad2ctWgI0kMYipGM+ttZqvNbJ2ZPfwN+9xqZivNbIWZTY9aP9DM1kZ+BpZWcJF49/HanQyfEqZV45pMHZpOnTNViomixHN6M0sCRgNXAplAhpnNcveVUfu0Bh4BLnH3PWbWOLK+PvAoEAIcWBA5dk/pPxSR+DFn/S6GTcmgZcMavDCsO3XPqhZ0JDkJsZwxpgPr3H2Du+cCM4Hriu0zHBh9tPDcfUdk/dXAO+6+O7LtHaB36UQXiU/zP9/NkEkZNKt3Fi8M6079GirFRBNLMTYBNkctZ0bWRWsDtDGzT8xsrpn1PoljMbMRZhY2s3BWVlbs6UXizIJNuxk0cT7n1k1m2vDuNKxZPehIcgpK6+JLFaA1cDnQDxhrZnVjPdjdx7h7yN1DjRo1KqVIIuVr0Rd7GDghg7NrJzNjeA8a10oOOpKcoliKcQvQLGq5aWRdtExglrvnufvnwBqKijKWY0US3tLMvQwYP5/6NaoxfXh3zq6tUkxksRRjBtDazFLNrBrQF5hVbJ/XKDpbxMwaUvTSegPwNnCVmdUzs3rAVZF1IhXG8i376D9uHnXOqsqMET04t86ZQUeS01TiVWl3zzezkRQVWhIwwd1XmNkoIOzus/hPAa4ECoCfuvsuADN7jKJyBRjl7rvL4oGIBGHl1v30Hz+PWslVmTG8B03qqhQrAnP3oDMcIxQKeTgcDjqGSIlWf3mAfmPnUi3pDF68qwctGtQIOpKcgJktcPdQLPtq5IvIKVi34wB3jJtLlTOMGSNUihWNilHkJK3POki/sfOAolJMbahSrGhUjCInYePOQ9w+di6Fhc6M4d05v1HNoCNJGVAxisRo8+5sbh87l9z8QqYN707rs2sFHUnKiOY/EolB5p5s+o6Zy6HcAqYP707bc2oHHUnKkM4YRUqwde9h+o2dy4GcPKYN60778+oEHUnKmIpR5AS+3JfD7WPnsvdQHlOHdqdDE5ViZaBiFPkGO/YXlWLWgSNMGpJOp2YxD/+XBKf3GEWOI+vAEW4fN48v9+cweUg63VrUCzqSlCOdMYoUs+vgEfqPm0fmnmwmDEojLaV+0JGknKkYRaLsOZTLHePmsXHXISYMTKNHywZBR5IA6KW0SMS+7Dz6j5/Hhp2HGDcgRM9WDYOOJAHRGaMIsO9wHndOmMfa7Qf5y53duLSNJkyuzFSMUukdyMlj4IT5rNq2n2fv6Mq3L2gcdCQJmIpRKrVDR/IZPDGD5Vv28efbu3LFhWcHHUnigN5jlEorOzefwZMyWLR5L8/068LV7c8JOpLECZ0xSqV0OLeAoZPChDfu5qnbOnNNx3ODjiRxRGeMUunk5BUwYmqYuZ/v4slbO9Gn03lBR5I4ozNGqVSO5Bdw19QFfLxuJ0/cdBE3dGkadCSJQypGqTRy8wu554WF/HtNFr+9oSO3hJqVfJBUSipGqRTyCgoZOX0hsz/bwf9c34G+6c2DjiRxTMUoFV5eQSE/nLGIf63czq/7tKd/jxZBR5I4F1MxmllvM1ttZuvM7OHjbB9kZllmtjjyMyxqW0HU+lmlGV6kJPkFhTzw4mL+sfxLfvH9dgzsmRJ0JEkAJV6VNrMkYDRwJZAJZJjZLHdfWWzXF9195HHu4rC7dz79qCInp6DQefDlJbyxdBuPfK8tw3q1DDqSJIhYzhjTgXXuvsHdc4GZwHVlG0vk9BQWOv/9ylJeW7yVn159AXdddn7QkSSBxFKMTYDNUcuZkXXF3WRmS83sFTOLvtyXbGZhM5trZtcf7xeY2YjIPuGsrKzY04scR2Gh88jflvHXhZk8cEUb7vt2q6AjSYIprYsvfwdS3P0i4B1gctS2Fu4eAm4Hnjazr/2v293HuHvI3UONGmlWEzl17s4vXl/Oi+HN/OA7rbj/itZBR5IEFEsxbgGizwCbRtZ9xd13ufuRyOI4oFvUti2RPzcAHwBdTiOvyDdydx6dtYLp877gnsvP58dXtgk6kiSoWIoxA2htZqlmVg3oCxxzddnMogea9gFWRdbXM7PqkdsNgUuA4hdtRE6bu/PYG6uYMmcTw3ul8t9XX4CZBR1LElSJV6XdPd/MRgJvA0nABHdfYWajgLC7zwJ+aGZ9gHxgNzAocng74C9mVkhRCT9+nKvZIqfF3fntPz5jwiefM/iSFH52TTuVopwWc/egMxwjFAp5OBwOOoYkCHfn92+v5tkP1nNnjxaMuq69SlGOy8wWRK53lEgjXyShPfXuWp79YD390pvz6z4qRSkdKkZJWH96by1/em8tt4aa8pvrO3DGGSpFKR2aj1ESjrvzf/9azej313NT16Y8fuNFKkUpVSpGSShHrz5P+ORz+qU315milAkVoySMwsKiD29Pn/cFg3qm8Oi1F+o9RSkTKkZJCPkFhfz3X5fyt4VbuPfy8/mpPqcoZUjFKHEvr6CQH81czJvLtvGTK9vwg+9qmJ+ULRWjxLWcvAJGTl/Iu6t28PNr2jH8Uk0dJmVPxShx63Bu0bf5fbR2J49d1547L04JOpJUEipGiUsHj+QzdFIG8zfu5ombL+JWfXGVlCMVo8SdfYfzGDRxPksz9/H0bZ25rvPxpv8UKTsqRokruw/lcuf4eazZfoBn7+jK1e3PCTqSVEIqRokbOw7k0H/cPDbtymbsgBCXX9A46EhSSakYJS5s3XuYO8bNY/v+HCYOSqNnq4ZBR5JKTMUogftiVza3j5vLvuw8pgxJJ5RSP+hIUsmpGCVQ67MOcsfYeeTkFzBteHcualo36EgiKkYJzmdf7qf/uHkAzBjeg3bn1g44kUgRFaMEYvmWffQfP4/qVc5g2rAetGpcM+hIIl9RMUq5W7BpD4Mmzqd2clWmD+9OiwY1go4kcgwVo5SrD9dkcfcLC2hcqzrThvegSd0zg44k8jX6agMpN39fspWhkzNoXv8sXrr7YpWixC2dMUq5mDpnI7+ctYK0FvUZOzBEnTOrBh1J5BupGKVMuTtPv7uWP763livaNebPt3cluWpS0LFETiiml9Jm1tvMVpvZOjN7+DjbB5lZlpktjvwMi9o20MzWRn4GlmZ4iW+Fhc6js1bwx/fWcnO3pjzfv5tKURJCiWeMZpYEjAauBDKBDDOb5e4ri+36oruPLHZsfeBRIAQ4sCBy7J5SSS9xKze/kB+/tJg3lm5jxKUteeR7bfVVBJIwYjljTAfWufsGd88FZgLXxXj/VwPvuPvuSBm+A/Q+taiSKA4dyWfo5AzeWLqNR77Xlp9d006lKAkllmJsAmyOWs6MrCvuJjNbamavmNnRWUVjOtbMRphZ2MzCWVlZMUaXeLTnUC53jJvHJ+t28sTNF3HXZecHHUnkpJXWx3X+DqS4+0UUnRVOPpmD3X2Mu4fcPdSoUaNSiiTlbevew9zylzms3Laf5/t306zbkrBiKcYtQPS/8KaRdV9x913ufiSyOA7oFuuxUjGs23GQm5/7lO37cpgyJJ2rNMGsJLBYijEDaG1mqWZWDegLzIrewczOjVrsA6yK3H4buMrM6plZPeCqyDqpQJZs3sstz39KbkEhM0b0oEfLBkFHEjktJV6Vdvd8MxtJUaElARPcfYWZjQLC7j4L+KGZ9QHygd3AoMixu83sMYrKFWCUu+8ug8chAflobRZ3TV1Ag5rVmDqkOykNNe5ZEp+5e9AZjhEKhTwcDgcdQ2LwxtKtPPDiYs5vVJMpQ9JpXDs56Egi38jMFrh7KJZ9NfJFTsnUuZv45evLCbWox7iBaRriJxWKilFOirvzp/fW8dS7a/hu26IhfmdW02gWqVhUjBKzwkLn139fweQ5m7ixaxN+d9NFVE3SBE1S8agYJSa5+YU8+PISZi3ZyvBeqTzyvXaccYZGs0jFpGKUEmXn5nP3Cwv5cE0WD/Vuy92XtdQQP6nQVIxyQnuzcxk8KYMlm/fyu5s6clta86AjiZQ5FaN8o237DjNg/Hw27c7m2Tu60buDRrNI5aBilONan3WQAePns+9wHpMHp3Px+RrNIpWHilG+ZmnmXgZNzMCAmSN60KFJnaAjiZQrFaMc45N1OxkxJUy9GtWYOrQ7qRriJ5WQilG+8taybfxo5mJSG9ZgytB0ztYQP6mkVIwCwLR5m/jFa8vp1rwe4wemUecsDfGTykvFWMm5O3+evY4/vLOG77RtzGgN8RNRMVZmhYXOqDdWMunTjdzYpQm/u1lD/ERAxVhp5RUU8tOXl/Da4q0M/VYqP79GQ/xEjlIxVkLZufncO20hH6zO4qdXX8C9l5+vIX4iUVSMlcze7FyGTMpg8ea9/PbGjvRL1xA/keJUjJXIl/tyGDBhHht3ZvPsHV3p3eHckg8SqYRUjJXEhqyD3Dl+Pnuzc5k0OI2erRoGHUkkbqkYK4FlmfsYNHE+ADNHXEzHphriJ3IiKsYK7tN1OxkxdQF1zqzK1KHptGxUM+hIInFPxViB/WPZNu6fuZiUhmcxZUh3zqmjIX4isYjp07xm1tvMVpvZOjN7+AT73WRmbmahyHKKmR02s8WRn+dLK7ic2Iz5X3Df9IV0bFqHl+66WKUochJKPGM0syRgNHAlkAlkmNksd19ZbL9awP3AvGJ3sd7dO5dSXimBu/PsB+v5/durufyCRjx3RzcN8RM5SbGcMaYD69x9g7vnAjOB646z32PA74CcUswnJ6Gw0HnsjVX8/u3VXN/5PMYOCKkURU5BLMXYBNgctZwZWfcVM+sKNHP3N49zfKqZLTKzf5tZr1OPKieSV1DIT15ewoRPPmfwJSk8eWtnjXsWOUWnffHFzM4AngQGHWfzNqC5u+8ys27Aa2bW3t33F7uPEcAIgObNNRLjZB3OLeC+6QuZ/dkOHryqDfd9u5WG+ImchlhOKbYAzaKWm0bWHVUL6AB8YGYbgR7ALDMLufsRd98F4O4LgPVAm+K/wN3HuHvI3UONGjU6tUdSSe3LzqP/+Hm8v3oHv7mhAyO/01qlKHKaYjljzABam1kqRYXYF7j96EZ33wd8NYzCzD4AHnT3sJkS4cWdAAANwElEQVQ1Ana7e4GZtQRaAxtKMX+ltnXvYQZPzODznYcYfXtXrumoIX4ipaHEYnT3fDMbCbwNJAET3H2FmY0Cwu4+6wSHXwqMMrM8oBC42913l0bwym7J5r0MmxLmcG4BEwencYmG+ImUGnP3oDMcIxQKeTgcDjpGXHtj6VZ+8tISGtWqzoRBabQ5u1bQkUTinpktcPdQLPtq5EsCcXeemb2OJ99ZQ6hFPf5yZzca1KwedCyRCkfFmCBy8gp46K9LeX3xVm7s0oTf3tSR6lX0GUWRsqBiTABZB45w19QwC7/Yqxm3RcqBijHOffblfoZOCrPr0BGeu6Mr39OVZ5Eyp2KMY7M/284Ppi+iZnIVXr6rp+ZRFCknKsY45O6M//hz/vetVVx4Xm3GDUjT7Dgi5UjFGGfyCgr55evLmTF/M73bn8OTt3XirGr6axIpT/ovLo7szc7l3mkL+XT9Lu69/HwevOoCfdezSABUjHFiQ9ZBhk4Os2XPYf5wSydu6tY06EgilZaKMQ58un4n97ywkKQzjGnDu5OWUj/oSCKVmooxYDPmf8H/e205qQ1rMH5gGs0bnBV0JJFKT8UYkIJC57dvrWLcx59zaZtG/Pn2LtROrhp0LBFBxRiIg0fyuX/GIt77bAeDeqbwi++3o4pm2xaJGyrGcpa5J5thk8Os3XGQx65rz50XpwQdSUSKUTGWo4Vf7GHElDBH8guZNDiNXq01W7lIPFIxlpPXF2/hp68s5ZzaycwcEaJVY82hKBKvVIxlzN156t21/Om9taSn1uf5/t2oX6Na0LFE5ARUjGUoJ6+AB19ewhtLt3FLt6b85oaOVKuiiywi8U7FWEZ27M9h+NQFLM3cy8Pfa8tdl7bUHIoiCULFWAZWbN3HsMlh9mbn8Xz/blzd/pygI4nISVAxlrJ3Vm7n/pmLqHNmVV6++2I6NNEciiKJRsVYStydMR9u4PF/fsZFTeowdkCIxrU1h6JIIlIxloLc/EJ+8doyXgpn8v2O5/J/t3TizGr6oiqRRBXTJVIz621mq81snZk9fIL9bjIzN7NQ1LpHIsetNrOrSyN0PNlzKJc7x8/jpXAmP/xOK57p10WlKJLgSjxjNLMkYDRwJZAJZJjZLHdfWWy/WsD9wLyodRcCfYH2wHnAu2bWxt0LSu8hBGfdjoMMnZzBtn05PH1bZ67v0iToSCJSCmI5Y0wH1rn7BnfPBWYC1x1nv8eA3wE5UeuuA2a6+xF3/xxYF7m/hPfx2p3c8OwnHDqSz4zhPVSKIhVILMXYBNgctZwZWfcVM+sKNHP3N0/22MjxI8wsbGbhrKysmIIH6YW5mxg4cT7n1TmTV++9hG4t6gUdSURK0WlffDGzM4AngUGneh/uPgYYAxAKhfx0M5WV/IJC/ufNVUz6dCPfaduYP/btTC3NoShS4cRSjFuAZlHLTSPrjqoFdAA+iIzsOAeYZWZ9Yjg2YRzIyeMHMxbxweoshn4rlZ9d044kfVGVSIUUSzFmAK3NLJWiUusL3H50o7vvAxoeXTazD4AH3T1sZoeB6Wb2JEUXX1oD80svfvnYvDuboZMz2JB1iP+9oSO3d28edCQRKUMlFqO755vZSOBtIAmY4O4rzGwUEHb3WSc4doWZvQSsBPKB+xLtinR4425GTF1AfkEhU4ak07NVw5IPEpGEZu7x9ZZeKBTycDgcdAwAXl2UyUOvLKNJvTMZPzBEy0Y1g44kIqfIzBa4e6jkPTXy5bgKC50/vLOa0e+v5+KWDXiuf1fqnqU5FEUqCxVjMYdzC/jxS4v5x/Iv6ZvWjFHXddAciiKVjIoxyvb9OQybHGb51n384vvtGPqtVM2hKFIJqRgjlm8pmkPxQE4eY+8MccWFZwcdSUQComIE/rl8Gw+8uIT6Narxyj09aXdu7aAjiUiAKnUxujvP/Xs9T/xzNZ2b1WXMgG40rqU5FEUqu0pbjEfyC3jkb8v428It9Ol0Hk/cfBHJVTVdmIhU0mLcfSiXu6aGydi4hweuaMMPv9tKF1lE5CuVrhjXbj/AkMkZ7Nh/hGf6deHaTucFHUlE4kylKsZ/r8li5LSFVK+axMwRPejSXNOFicjXVZpinPzpRn799xVccE5txg0M0aTumUFHEpE4VeGLMb+gkFFvrGTKnE1c0e5s/ti3MzWqV/iHLSKnoUI3xL7DeYycvpCP1u5kxKUteah3W82hKCIlqrDFuGnXIYZMymDTrmx+d1NHbkvTHIoiEpsKWYzzP9/NXVPDODB1aHcuPr9B0JFEJIFUuGJ8ObyZn726jGb1z2LCwDRSGtYIOpKIJJgKU4yFhc4Tb6/m+X+v55JWDXj29m7UOUtfVCUiJ69CFGN2bj4/mrmYf63czh3dm/OrPu2pmqQ5FEXk1CR8MW7bd5hhk8Os2rafR6+9kEE9UzS8T0ROS8IX469mrWDTrmzGD0rj2xc0DjqOiFQACV+M/3N9R3YfyuWCc2oFHUVEKoiEL8ZGtarTqFb1oGOISAUS0xUKM+ttZqvNbJ2ZPXyc7Xeb2TIzW2xmH5vZhZH1KWZ2OLJ+sZk9X9oPQESktJV4xmhmScBo4EogE8gws1nuvjJqt+nu/nxk/z7Ak0DvyLb17t65dGOLiJSdWM4Y04F17r7B3XOBmcB10Tu4+/6oxRqAl15EEZHyFUsxNgE2Ry1nRtYdw8zuM7P1wBPAD6M2pZrZIjP7t5n1Oq20IiLloNQ+Be3uo939fOAh4BeR1duA5u7eBfgxMN3MvvYVfGY2wszCZhbOysoqrUgiIqcklmLcAjSLWm4aWfdNZgLXA7j7EXffFbm9AFgPtCl+gLuPcfeQu4caNWoUa3YRkTIRSzFmAK3NLNXMqgF9gVnRO5hZ66jF7wNrI+sbRS7eYGYtgdbAhtIILiJSVkq8Ku3u+WY2EngbSAImuPsKMxsFhN19FjDSzK4A8oA9wMDI4ZcCo8wsDygE7nb33WXxQERESou5x9cF5FAo5OFwOOgYIlLBmNkCdw/Fsq+moBERKSbuzhjNLAvYdJKHNQR2lkGc8pLI+RM5OyR2/kTODuWfv4W7x3R1N+6K8VSYWTjWU+R4lMj5Ezk7JHb+RM4O8Z1fL6VFRIpRMYqIFFNRinFM0AFOUyLnT+TskNj5Ezk7xHH+CvEeo4hIaaooZ4wiIqVGxSgiUkzcF2NJs4dH9rnVzFaa2Qozmx61fqCZrY38DDzesWXpNLMXRM18Put4x5a1GGZufyoq4xoz2xu1La6f+xKyJ8Jz39zM3o9M6bfUzK6J2vZI5LjVZnZ1+SY/9exxNeO/u8ftD0Vjs9cDLYFqwBLgwmL7tAYWAfUiy40jf9anaMKK+kC9yO16iZA9cvtgvD/3xfb/AUXj6BPiuf+m7Iny3FN04eKeyO0LgY1Rt5cA1YHUyP0kJUj2FGB5kM/90Z94P2MscfZwYDgw2t33ALj7jsj6q4F33H13ZNs7/OfrFsrD6WSPB7Hkj9YPmBG5nQjPfbTo7PEglvwOHJ3btA6wNXL7OmCmF0359zmwLnJ/5eV0sseNeC/GWGYPbwO0MbNPzGyumfU+iWPL0ulkB0iOTN4718yuL+uwxxHz82dmLSg6O5l9sseWkdPJDonx3P8K6G9mmcBbFJ31xnpsWTqd7BAnM/4n/NenUvQYWgOXUzSJ7odm1jHQRLE7bnZ330vRuM4tkXksZ5vZMndfH2DWE+kLvOLuBUEHOQXHy54Iz30/YJK7/8HMLgammlmHoEPF6JuyH53xf5eZdQNeM7P2fux3SpWLeD9jjGX28ExglrvnRV46rKGobE525vHSdjrZcfctkT83AB8AXco6cDEn8/z15diXoonw3B9VPHuiPPdDgZcA3H0OkEzRpAyJ8NwfN7vHOON/uQj6Tc4S3sitQtEb96n8543c9sX26Q1MjtxuSNFpfAOK3vj/nKI3/+tFbtdPkOz1gOpR69dygosHQeWP7NcW2EhksEBkXdw/9yfInhDPPfAPYFDkdjuK3qczoD3HXnzZQPlefDmd7I2OZqXo4s2W8vx3c0zGIH7pST7R11B0JrUe+Hlk3SigT+S2UfQ91iuBZUDfqGOHUPTm8zpgcKJkB3pGlpdE/hwaj899ZPlXwOPHOTaun/tvyp4ozz1FV3M/ieRcDFwVdezPI8etBr6XKNmBm4AVkXULgWuDeO7dXUMCRUSKi/f3GEVEyp2KUUSkGBWjiEgxKkYRkWJUjCIixagYpUKLzNiyPHL7cjN7I+hMEv9UjBKXrIj+fUog9A9P4kbk7G61mU0BlgN3mtkcM1toZi+bWc3Ifmlm9qmZLTGz+WZWK3LsR5F9F5pZz2AfjSSyijCJhFQsrYGBFI2Y+RtwhbsfMrOHgB+b2ePAi8Bt7p5hZrWBw8AO4Ep3zzGz1hSNf47L7yyW+KdilHizyd3nmtl/ERk6ZmZQNO52DnABsM3dMwA8MvOKmdUA/mxmnYECgpp8QCoEFaPEm0ORP42iyW77RW88wZRyDwDbgU4UvUWUU2YJpcLTe4wSr+YCl5hZKyg6IzSzNhRNjHCumaVF1tcysyoUzQS9zd0LgTspmmJf5JSoGCUuuXsWMAiYYWZLKXoZ3daLpsu/DXjGzJZQ9LUJycCzwMDIurb858xT5KRpdh0RkWJ0xigiUoyKUUSkGBWjiEgxKkYRkWJUjCIixagYRUSKUTGKiBTz/wEjXBSIADaxCwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x114beaeb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "t2_results.sort_values([\"recall\", \"precision\"]).plot(\"recall\", \"precision\", \"line\", figsize=(5,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trial 2 Notes:\n",
    "Welp - that didn't work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
