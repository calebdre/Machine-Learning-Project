{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building\n",
    "\n",
    "## Which algorithm to use?\n",
    "We'll use a random forest classifier (rfc) with bootstrapping and feature bagging optimizations because:\n",
    "- ease of implementation\n",
    "- rfcs handle multi-class predictions well without more additional effort\n",
    "- works well with high dimensional data\n",
    "- we'll choose use random forest as opposed to boosted trees since we have highly dimensional data\n",
    "- with a reasonably high probability, can be used with the other datasets for this project since the algorithm is very robust\n",
    "\n",
    "## The Algorithm\n",
    "We'll use the CART algorithm for splitting since we have continuous data.  \n",
    "  \n",
    "[Full example](https://machinelearningmastery.com/classification-and-regression-trees-for-machine-learning/)  \n",
    "  \n",
    "Steps:\n",
    "1. Initialize Tree\n",
    "2. For each column, calc best split across all rows based using gini impurity score - [exmplanation](https://en.wikipedia.org/wiki/Decision_tree_learning#Gini_impurity) | [exmaple](https://www.researchgate.net/post/How_to_compute_impurity_using_Gini_Index) | [useful blog](http://dni-institute.in/blogs/cart-algorithm-for-decision-tree/)\n",
    "3. Split the dataset based on the split condition with the highest gini score and add both sets as leaves on a tree node. The node represents a decision point, that being the condition with the highest gini score.\n",
    "3. Repeat 2 & 3 until an arbitrary minimum number of rows are left\n",
    "4. Prune tree\n",
    "\n",
    "idea: instead of using the raw values, categorize the numbers as # of stds away from mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_squared_probaility = lambda x, total: (x.count() / total) ** 2\n",
    "calc_split_gini = lambda split, total: split.apply(calc_squared_probability, {total: total}).sum()\n",
    "\n",
    "def calc_best_gini_split(df, labels):\n",
    "    total_rows = df.size\n",
    "    grouped = labels.merge(df, left_index=True,right_index=True).groupby(\"label\")\n",
    "    ginis = {}\n",
    "    for index, row in df.iterrows():\n",
    "        split1 = grouped[row > df]\n",
    "        split2 = grouped[row < df]\n",
    "        ginis[index] = np.sum(calc_split_gini(split1, total_rows) / total_rows, calc_split_gini(split2, total_rows) / total_rows)\n",
    "    \n",
    "    return df.iloc[ginis.values().index(ginis.values().max())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
