{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%store -r\n",
    "from time import time\n",
    "from math import sqrt, floor\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from time import time\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "import pandas as pd\n",
    "from multiprocessing.dummy import Pool as ThreadPool \n",
    "from IPython.core.debugger import set_trace\n",
    "import pickle\n",
    "import os\n",
    "from rfc import RandomForest\n",
    "pd.options.display.float_format = '{:.3f}'.format\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use = \"default\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"cleaned_testData1.csv\")\n",
    "labels = pd.read_csv(\"cleaned_trainLabel1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.drop(train.columns[0], axis=1)\n",
    "labels = labels.drop(labels.columns[0], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f0</th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "      <th>f6</th>\n",
       "      <th>f7</th>\n",
       "      <th>f8</th>\n",
       "      <th>f9</th>\n",
       "      <th>...</th>\n",
       "      <th>f151</th>\n",
       "      <th>f152</th>\n",
       "      <th>f153</th>\n",
       "      <th>f154</th>\n",
       "      <th>f155</th>\n",
       "      <th>f156</th>\n",
       "      <th>f157</th>\n",
       "      <th>f158</th>\n",
       "      <th>f159</th>\n",
       "      <th>f160</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>149.000</td>\n",
       "      <td>149.000</td>\n",
       "      <td>149.000</td>\n",
       "      <td>149.000</td>\n",
       "      <td>149.000</td>\n",
       "      <td>149.000</td>\n",
       "      <td>149.000</td>\n",
       "      <td>149.000</td>\n",
       "      <td>149.000</td>\n",
       "      <td>149.000</td>\n",
       "      <td>...</td>\n",
       "      <td>149.000</td>\n",
       "      <td>149.000</td>\n",
       "      <td>149.000</td>\n",
       "      <td>149.000</td>\n",
       "      <td>149.000</td>\n",
       "      <td>149.000</td>\n",
       "      <td>149.000</td>\n",
       "      <td>149.000</td>\n",
       "      <td>149.000</td>\n",
       "      <td>149.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.513</td>\n",
       "      <td>1.204</td>\n",
       "      <td>1.262</td>\n",
       "      <td>1.888</td>\n",
       "      <td>1.390</td>\n",
       "      <td>1.668</td>\n",
       "      <td>1.305</td>\n",
       "      <td>1.743</td>\n",
       "      <td>2.567</td>\n",
       "      <td>1.913</td>\n",
       "      <td>...</td>\n",
       "      <td>1.562</td>\n",
       "      <td>2.009</td>\n",
       "      <td>1.751</td>\n",
       "      <td>2.210</td>\n",
       "      <td>2.285</td>\n",
       "      <td>2.707</td>\n",
       "      <td>2.787</td>\n",
       "      <td>1.225</td>\n",
       "      <td>1.920</td>\n",
       "      <td>2.801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.410</td>\n",
       "      <td>0.404</td>\n",
       "      <td>0.437</td>\n",
       "      <td>0.342</td>\n",
       "      <td>0.460</td>\n",
       "      <td>0.560</td>\n",
       "      <td>0.370</td>\n",
       "      <td>0.333</td>\n",
       "      <td>0.119</td>\n",
       "      <td>0.209</td>\n",
       "      <td>...</td>\n",
       "      <td>0.446</td>\n",
       "      <td>0.163</td>\n",
       "      <td>0.298</td>\n",
       "      <td>0.124</td>\n",
       "      <td>0.199</td>\n",
       "      <td>0.142</td>\n",
       "      <td>0.171</td>\n",
       "      <td>0.369</td>\n",
       "      <td>0.294</td>\n",
       "      <td>0.134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.257</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>2.349</td>\n",
       "      <td>1.343</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.651</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.894</td>\n",
       "      <td>1.829</td>\n",
       "      <td>2.389</td>\n",
       "      <td>2.391</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.483</td>\n",
       "      <td>2.493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.231</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.676</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.315</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.586</td>\n",
       "      <td>2.493</td>\n",
       "      <td>1.793</td>\n",
       "      <td>...</td>\n",
       "      <td>1.031</td>\n",
       "      <td>1.897</td>\n",
       "      <td>1.583</td>\n",
       "      <td>2.114</td>\n",
       "      <td>2.154</td>\n",
       "      <td>2.599</td>\n",
       "      <td>2.678</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.758</td>\n",
       "      <td>2.727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.475</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.839</td>\n",
       "      <td>1.252</td>\n",
       "      <td>1.652</td>\n",
       "      <td>1.198</td>\n",
       "      <td>1.727</td>\n",
       "      <td>2.549</td>\n",
       "      <td>1.896</td>\n",
       "      <td>...</td>\n",
       "      <td>1.597</td>\n",
       "      <td>2.005</td>\n",
       "      <td>1.723</td>\n",
       "      <td>2.197</td>\n",
       "      <td>2.262</td>\n",
       "      <td>2.707</td>\n",
       "      <td>2.785</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.863</td>\n",
       "      <td>2.780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.659</td>\n",
       "      <td>1.204</td>\n",
       "      <td>1.361</td>\n",
       "      <td>2.026</td>\n",
       "      <td>1.570</td>\n",
       "      <td>1.842</td>\n",
       "      <td>1.455</td>\n",
       "      <td>1.853</td>\n",
       "      <td>2.609</td>\n",
       "      <td>1.996</td>\n",
       "      <td>...</td>\n",
       "      <td>1.872</td>\n",
       "      <td>2.103</td>\n",
       "      <td>1.866</td>\n",
       "      <td>2.283</td>\n",
       "      <td>2.386</td>\n",
       "      <td>2.798</td>\n",
       "      <td>2.897</td>\n",
       "      <td>1.327</td>\n",
       "      <td>1.955</td>\n",
       "      <td>2.847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3.027</td>\n",
       "      <td>2.713</td>\n",
       "      <td>2.862</td>\n",
       "      <td>2.984</td>\n",
       "      <td>2.754</td>\n",
       "      <td>3.507</td>\n",
       "      <td>2.306</td>\n",
       "      <td>2.760</td>\n",
       "      <td>2.986</td>\n",
       "      <td>2.573</td>\n",
       "      <td>...</td>\n",
       "      <td>2.658</td>\n",
       "      <td>2.861</td>\n",
       "      <td>2.856</td>\n",
       "      <td>2.579</td>\n",
       "      <td>2.945</td>\n",
       "      <td>3.285</td>\n",
       "      <td>3.274</td>\n",
       "      <td>2.728</td>\n",
       "      <td>3.050</td>\n",
       "      <td>3.263</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 161 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           f0      f1      f2      f3      f4      f5      f6      f7      f8  \\\n",
       "count 149.000 149.000 149.000 149.000 149.000 149.000 149.000 149.000 149.000   \n",
       "mean    1.513   1.204   1.262   1.888   1.390   1.668   1.305   1.743   2.567   \n",
       "std     0.410   0.404   0.437   0.342   0.460   0.560   0.370   0.333   0.119   \n",
       "min     1.000   1.000   1.000   1.257   1.000   1.000   1.000   1.000   2.349   \n",
       "25%     1.231   1.000   1.000   1.676   1.000   1.315   1.000   1.586   2.493   \n",
       "50%     1.475   1.000   1.000   1.839   1.252   1.652   1.198   1.727   2.549   \n",
       "75%     1.659   1.204   1.361   2.026   1.570   1.842   1.455   1.853   2.609   \n",
       "max     3.027   2.713   2.862   2.984   2.754   3.507   2.306   2.760   2.986   \n",
       "\n",
       "           f9   ...      f151    f152    f153    f154    f155    f156    f157  \\\n",
       "count 149.000   ...   149.000 149.000 149.000 149.000 149.000 149.000 149.000   \n",
       "mean    1.913   ...     1.562   2.009   1.751   2.210   2.285   2.707   2.787   \n",
       "std     0.209   ...     0.446   0.163   0.298   0.124   0.199   0.142   0.171   \n",
       "min     1.343   ...     1.000   1.651   1.000   1.894   1.829   2.389   2.391   \n",
       "25%     1.793   ...     1.031   1.897   1.583   2.114   2.154   2.599   2.678   \n",
       "50%     1.896   ...     1.597   2.005   1.723   2.197   2.262   2.707   2.785   \n",
       "75%     1.996   ...     1.872   2.103   1.866   2.283   2.386   2.798   2.897   \n",
       "max     2.573   ...     2.658   2.861   2.856   2.579   2.945   3.285   3.274   \n",
       "\n",
       "         f158    f159    f160  \n",
       "count 149.000 149.000 149.000  \n",
       "mean    1.225   1.920   2.801  \n",
       "std     0.369   0.294   0.134  \n",
       "min     1.000   1.483   2.493  \n",
       "25%     1.000   1.758   2.727  \n",
       "50%     1.000   1.863   2.780  \n",
       "75%     1.327   1.955   2.847  \n",
       "max     2.728   3.050   3.263  \n",
       "\n",
       "[8 rows x 161 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>149.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        label\n",
       "count 149.000\n",
       "mean    1.604\n",
       "std     1.096\n",
       "min     1.000\n",
       "25%     1.000\n",
       "50%     1.000\n",
       "75%     2.000\n",
       "max     5.000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(df, labels):\n",
    "    return labels.merge(df, left_index=True,right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mprint(*args):\n",
    "    for arg in args:\n",
    "        print(arg)\n",
    "        print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---  \n",
    "\n",
    "# Model Building\n",
    "\n",
    "## Which algorithm to use?\n",
    "We'll use a **random forest classifier** (rfc) with bootstrapping and feature bagging optimizations because:\n",
    "- ease of implementation\n",
    "- rfcs handle multi-class predictions well without more additional effort\n",
    "- works well with high dimensional data\n",
    "- we'll choose use random forest as opposed to boosted trees since we have highly dimensional data\n",
    "- with a reasonably high probability, can be used with the other datasets for this project since the algorithm is very robust\n",
    "\n",
    "## The Algorithm\n",
    "We'll use the CART algorithm for splitting since we have continuous data.  \n",
    "  \n",
    "[Full example](https://machinelearningmastery.com/classification-and-regression-trees-for-machine-learning/)  \n",
    "  \n",
    "Steps:\n",
    "1. Initialize Tree\n",
    "2. For each column, calc best split across all rows based using gini impurity score - [exmplanation](https://en.wikipedia.org/wiki/Decision_tree_learning#Gini_impurity) | [exmaple](https://www.researchgate.net/post/How_to_compute_impurity_using_Gini_Index) | [useful blog](http://dni-institute.in/blogs/cart-algorithm-for-decision-tree/)\n",
    "3. Split the dataset based on the split condition with the highest gini score and add both sets as leaves on a tree node. The node represents a decision point, that being the condition with the highest gini score.\n",
    "3. Repeat 2 & 3 until an arbitrary minimum number of rows are left\n",
    "4. Prune tree\n",
    "\n",
    "ideas:\n",
    "- instead of using the raw values, categorize the numbers as # of stds away from mean\n",
    "- > Alternatively, the random forest can apply weight concept for considering the impact of result from any decision tree. Tree with high error rate are given low weight value and vise versa. This would increase the decision impact of trees with low error rate - [medium post](https://medium.com/machine-learning-101/chapter-5-random-forest-classifier-56dc7425c3e1)\n",
    "- [parameters to  tune](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)\n",
    "https://www.analyticsvidhya.com/blog/2016/04/complete-tutorial-tree-based-modeling-scratch-in-python/\n",
    "- https://www.analyticsvidhya.com/blog/2015/06/tuning-random-forest-model/\n",
    "- https://stats.stackexchange.com/questions/260460/optimization-of-a-random-forest-model\n",
    "- https://followthedata.wordpress.com/2012/06/02/practical-advice-for-machine-learning-bias-variance/\n",
    "- https://towardsdatascience.com/beyond-accuracy-precision-and-recall-3da06bea9f6c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---  \n",
    "# Model Training & Tuning\n",
    "## Context\n",
    "Now that we have our classifier, let's think about how we're going to train the model. \n",
    "\n",
    "We'll also measure performance through [precision](https://en.wikipedia.org/wiki/Precision_and_recall) & [recall](https://towardsdatascience.com/beyond-accuracy-precision-and-recall-3da06bea9f6c) - it tells us, for each class, how well the model identifies all cases of that class (recall) and how well it can correctly classify those cases (precision). From wikipedia:\n",
    "> Suppose a computer program for recognizing dogs in photographs identifies eight dogs in a picture containing 12 dogs and some cats. Of the eight dogs identified, five actually are dogs (true positives), while the rest are cats (false positives). The program's precision is 5/8 while its recall is 5/12.\n",
    "\n",
    "![precision & recall formulas](https://cdn-images-1.medium.com/max/2000/1*6NkN_LINs2erxgVJ9rkpUA.png)\n",
    "We can use the [f1 score](https://en.wikipedia.org/wiki/F1_score) to maximize precision and recall when testing different models.  \n",
    "![f1 score formula](https://cdn-images-1.medium.com/max/1600/1*UJxVqLnbSj42eRhasKeLOA.png)\n",
    "\n",
    "Recall and precision seem to be very related to bias and variance of the model, so we can maximize the f1 score by tuning the model to affect these.\n",
    "#### Minimizing bias\n",
    "- use new/different features\n",
    "- increase the size of the trees (increases variance)\n",
    "- increase the number of trees in the forest\n",
    "\n",
    "#### Minimizing variance\n",
    "- decrease the number of features\n",
    "    + probably want to aim to features that are correlated and/or collapse the overall number of features through PCA\n",
    "- use more data for each tree  \n",
    "\n",
    "  \n",
    "Beware: too much completixy is bad & not enough complexity is also bad  \n",
    "![bias variance tradeoff](http://scott.fortmann-roe.com/docs/docs/BiasVariance/biasvariance.png)  \n",
    "  \n",
    "\n",
    "#### Stability\n",
    "We need to make sure to train the classifier on as many data points as possible while also leaving enough to test to reliably tell how well the classifier actually performs. We'll use [k-fold cross validation](https://www.analyticsvidhya.com/blog/2015/11/improve-model-performance-cross-validation-in-python-r/):  \n",
    "  \n",
    "> 1. Randomly split your entire dataset into k ”folds”.\n",
    "2. For each k folds in your dataset, build your model on k – 1 folds of the data set. Then, test the model to check the effectiveness for kth fold.\n",
    "3. Record the error you see on each of the predictions.\n",
    "4. Repeat this until each of the k folds has served as the test set.\n",
    "\n",
    "## Procedure\n",
    "1. Record and save an input configuration for the random forest\n",
    "1. Separate data into k folds\n",
    "2. For each fold *k*: \n",
    "    1. train the classifier on k-1 folds\n",
    "    2. predict the k-th fold\n",
    "    3. measure the: accuracy, [logarithmic](http://wiki.fast.ai/index.php/Log_Loss) [loss](https://towardsdatascience.com/metrics-to-evaluate-your-machine-learning-algorithm-f10ba6e38234#f217), recall, precision, and f1-score\n",
    "3. Record the performance measures & associate it with the input configuration\n",
    "3. Evaluate the overall performance difference across all configurations\n",
    "4. Change variables from the input configuration that optimizes model perfomance & repeat steps 1-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Experiment:\n",
    "    def __init__(self, df, labels, model_class, model_config_names, model_config_init_values, k_folds=10):        \n",
    "        self.data = df\n",
    "        self.labels = labels\n",
    "        self.label_values = labels[\"label\"].unique()\n",
    "        self.model_class = model_class\n",
    "        self.model_config_names = model_config_names\n",
    "        \n",
    "        self.trial_num = 1\n",
    "        self.init_experiments(model_config_names, model_config_init_values)\n",
    "        \n",
    "        self.k_folds = k_folds\n",
    "    \n",
    "    def tweak(self, parameter, new_value):\n",
    "        self.experiments.loc[len(self.experiments) - 1][parameter] = new_value\n",
    "        return self\n",
    "    \n",
    "    def run_trial(self):\n",
    "        exp_num = self.experiments.shape[0]\n",
    "        mprint(\"Running trail #{}\\n------------------------------------\".format(exp_num))\n",
    "\n",
    "        performance_results = []\n",
    "        folds, labels = self.split_k_folds()\n",
    "        \n",
    "        config = self.current_model_config()\n",
    "        labels = pd.DataFrame(labels)\n",
    "        \n",
    "        for i, test_train in enumerate(folds):\n",
    "            mprint(\n",
    "                \"*************************\",\n",
    "                \"Running fold {} of {}\".format(i+1, self.k_folds),\n",
    "                \"*************************\"\n",
    "            )\n",
    "            \n",
    "            model = self.model()\n",
    "            t1 = time()\n",
    "            model.train(test_train[1], pd.DataFrame(labels), *config)\n",
    "            mprint(\"fold {} took {}s\".format(i+1, time() - t1))\n",
    "            performance = self.measure_performance(model, merge(test_train[0], labels), self.label_values)\n",
    "            \n",
    "            self.record_trial(performance, test_train[1], test_train[0], i+1 == len(folds))\n",
    "        \n",
    "        self.trial_num += 1\n",
    "        self.experiments.loc[len(self.experiments) - 1][\"trial_num\"] = self.trial_num\n",
    "        return self.trial_results()\n",
    "        \n",
    "    def trial_results(self, trial_num=None):\n",
    "        if trial_num == None:\n",
    "            trial_num = self.experiments[\"trial_num\"].max() - 1\n",
    "        \n",
    "        return self.experiments[self.experiments[\"trial_num\"] == trial_num], self.experiment_data[int(trial_num)]    \n",
    "\n",
    "#<--------  PRIVATE METHODS -------->\n",
    "    def current_trial(self):\n",
    "        return self.get_trial(\"current\", \"latest\")\n",
    "    \n",
    "    def current_model_config(self):\n",
    "        return self.experiments.loc[len(self.experiments) - 1][self.model_config_names].astype(\"int32\").values\n",
    "    \n",
    "    def get_trial(self, trial_num=None, trial_instance=None):\n",
    "        if trial_num == \"current\":\n",
    "            if trial_instance == None:\n",
    "                return self.experiments[self.experiments[\"trial_num\"] == self.experiments[\"trial_num\"].max()]\n",
    "            elif trial_instance == \"latest\":\n",
    "                latest = self.experiments[self.experiments[\"trial_num\"] == self.experiments[\"trial_num\"].max()]\n",
    "                return latest.loc[len(latest) - 1]\n",
    "        elif trial_num == \"prev\":\n",
    "            if trial_instance != None:\n",
    "                return self.experiment[self.experiments[\"trial_num\"].max() - 1].iloc[trial_instance]\n",
    "            else:\n",
    "                return self.experiment[self.experiments[\"trial_num\"].max() - 1]\n",
    "        elif trial_num == None:\n",
    "            return self.experiments[self.experiments[\"trial_num\"] == self.experiments[\"trial_num\"].max()]\n",
    "        else:\n",
    "            return self.experiments[self.experiments[\"trial_num\"] == trial_num]\n",
    "        \n",
    "    def prev_trial(self):\n",
    "        return self.get_trial(\"prev\")\n",
    "    \n",
    "    def trial_data(self, trial_num=None):\n",
    "        if trial_num == None:\n",
    "            trial_num = self.experiments[\"trial_num\"].max()-1\n",
    "        \n",
    "        return self.experiment_data[trial_num]\n",
    "    \n",
    "    def model(self):\n",
    "        return self.model_class(verbose=False)\n",
    "    \n",
    "    def init_experiments(self, config_names, config_values):\n",
    "        derived_cols = self.performance_measures() + [\"trial_num\"]\n",
    "        all_cols = config_names + derived_cols\n",
    "        first_row = config_values + [np.nan for i in derived_cols]\n",
    "        \n",
    "        self.experiments = pd.DataFrame(columns=all_cols)\n",
    "        self.experiments.loc[0] = first_row\n",
    "        self.experiments.loc[0][\"trial_num\"] = self.trial_num\n",
    "        self.experiments[config_names] = self.experiments[config_names].fillna(-1)\n",
    "        self.experiment_data = []\n",
    "    \n",
    "    def record_trial(self, results, train, test, final=False):\n",
    "        for key in results:\n",
    "            self.experiments.loc[len(self.experiments) - 1][key] = results[key]\n",
    "\n",
    "        self.experiments.loc[len(self.experiments)] = self.experiments.loc[len(self.experiments) - 1]\n",
    "        self.experiment_data.append((train, test))\n",
    "            \n",
    "    def performance_measures(self):\n",
    "        return [\"log_loss\", \"class_accuracy\", \"precision\", \"recall\", \"f1-score\", \"support\"]\n",
    "    \n",
    "    def split_k_folds(self):\n",
    "        splitter = int(np.ceil(self.data.shape[0] / self.k_folds))\n",
    "        df = shuffle(merge(self.data, self.labels))\n",
    "        labels = df.pop(\"label\")\n",
    "\n",
    "        folds = []\n",
    "        for i in range(1, self.k_folds+1):\n",
    "            train = df.iloc[(i-1) * splitter: i * splitter]\n",
    "            test = df.iloc[np.r_[0:(i-1) * splitter, i*splitter: df.shape[0]]]\n",
    "            folds.append((train, test))\n",
    "\n",
    "        return folds, labels\n",
    "    \n",
    "    def measure_performance(self, model, test_set, label_values=None):\n",
    "        test_labels = test_set.pop(\"label\")\n",
    "        if label_values is None:\n",
    "            label_values = test_labels.unique()\n",
    "\n",
    "        predictions = test_set.apply(lambda row: model.predict(row), axis=1)\n",
    "        precision, recall, fscore, support = score(test_labels, predictions, average='weighted')\n",
    "\n",
    "        lvs = [[1 if p == 1 else 0 for l in label_values] for p in predictions]\n",
    "        return {\n",
    "            \"log_loss\" : log_loss(test_labels, lvs, normalize=True, labels=label_values),\n",
    "            \"class_accuracy\": accuracy_score(test_labels, predictions, normalize=True),\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1-score\": fscore,\n",
    "            \"support\": support\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_config_names = [\"num_trees\", \"num_features\", \"num_sample_rows\", \"max_tree_depth\", \"min_split_samples\"]\n",
    "init_config_values = [   10,          None,           None,              20,                5] # default settings = initial settings\n",
    "\n",
    "e = Experiment(train, labels, RandomForest, init_config_names, init_config_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10, -1, -1, 20,  5], dtype=int32)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e.current_model_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running trail #1\n",
      "------------------------------------\n",
      " \n",
      "*************************\n",
      " \n",
      "Running fold 1 of 10\n",
      " \n",
      "*************************\n",
      " \n",
      "fold 1 took 21.575438976287842s\n",
      " \n",
      "*************************\n",
      " \n",
      "Running fold 2 of 10\n",
      " \n",
      "*************************\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/python3/3.5.0/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 2 took 17.12821912765503s\n",
      " \n",
      "*************************\n",
      " \n",
      "Running fold 3 of 10\n",
      " \n",
      "*************************\n",
      " \n",
      "fold 3 took 19.819180011749268s\n",
      " \n",
      "*************************\n",
      " \n",
      "Running fold 4 of 10\n",
      " \n",
      "*************************\n",
      " \n",
      "fold 4 took 19.02623200416565s\n",
      " \n",
      "*************************\n",
      " \n",
      "Running fold 5 of 10\n",
      " \n",
      "*************************\n",
      " \n",
      "fold 5 took 23.62216305732727s\n",
      " \n",
      "*************************\n",
      " \n",
      "Running fold 6 of 10\n",
      " \n",
      "*************************\n",
      " \n",
      "fold 6 took 17.78378915786743s\n",
      " \n",
      "*************************\n",
      " \n",
      "Running fold 7 of 10\n",
      " \n",
      "*************************\n",
      " \n",
      "fold 7 took 18.721735954284668s\n",
      " \n",
      "*************************\n",
      " \n",
      "Running fold 8 of 10\n",
      " \n",
      "*************************\n",
      " \n",
      "fold 8 took 22.229801893234253s\n",
      " \n",
      "*************************\n",
      " \n",
      "Running fold 9 of 10\n",
      " \n",
      "*************************\n",
      " \n",
      "fold 9 took 19.35861110687256s\n",
      " \n",
      "*************************\n",
      " \n",
      "Running fold 10 of 10\n",
      " \n",
      "*************************\n",
      " \n",
      "fold 10 took 28.434226989746094s\n",
      " \n",
      "Warning:e is <__main__.Experiment object at 0x10c0a66d8>\n",
      "Proper storage of interactively declared classes (or instances\n",
      "of those classes) is not possible! Only instances\n",
      "of classes in real modules on file system can be %store'd.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "e.run_trial()\n",
    "%store e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "perf, train_test = e.trial_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_trees</th>\n",
       "      <th>num_features</th>\n",
       "      <th>num_sample_rows</th>\n",
       "      <th>max_tree_depth</th>\n",
       "      <th>min_split_samples</th>\n",
       "      <th>log_loss</th>\n",
       "      <th>class_accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "      <th>trial_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>20.000</td>\n",
       "      <td>5.000</td>\n",
       "      <td>1.609</td>\n",
       "      <td>0.867</td>\n",
       "      <td>0.751</td>\n",
       "      <td>0.867</td>\n",
       "      <td>0.805</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>20.000</td>\n",
       "      <td>5.000</td>\n",
       "      <td>1.609</td>\n",
       "      <td>0.733</td>\n",
       "      <td>0.538</td>\n",
       "      <td>0.733</td>\n",
       "      <td>0.621</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>20.000</td>\n",
       "      <td>5.000</td>\n",
       "      <td>1.609</td>\n",
       "      <td>0.667</td>\n",
       "      <td>0.444</td>\n",
       "      <td>0.667</td>\n",
       "      <td>0.533</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>20.000</td>\n",
       "      <td>5.000</td>\n",
       "      <td>1.609</td>\n",
       "      <td>0.800</td>\n",
       "      <td>0.640</td>\n",
       "      <td>0.800</td>\n",
       "      <td>0.711</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>20.000</td>\n",
       "      <td>5.000</td>\n",
       "      <td>1.609</td>\n",
       "      <td>0.800</td>\n",
       "      <td>0.640</td>\n",
       "      <td>0.800</td>\n",
       "      <td>0.711</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>20.000</td>\n",
       "      <td>5.000</td>\n",
       "      <td>1.609</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0.160</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0.229</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>20.000</td>\n",
       "      <td>5.000</td>\n",
       "      <td>1.609</td>\n",
       "      <td>0.667</td>\n",
       "      <td>0.444</td>\n",
       "      <td>0.667</td>\n",
       "      <td>0.533</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>20.000</td>\n",
       "      <td>5.000</td>\n",
       "      <td>1.609</td>\n",
       "      <td>0.733</td>\n",
       "      <td>0.538</td>\n",
       "      <td>0.733</td>\n",
       "      <td>0.621</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>20.000</td>\n",
       "      <td>5.000</td>\n",
       "      <td>1.609</td>\n",
       "      <td>0.667</td>\n",
       "      <td>0.444</td>\n",
       "      <td>0.667</td>\n",
       "      <td>0.533</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>20.000</td>\n",
       "      <td>5.000</td>\n",
       "      <td>1.609</td>\n",
       "      <td>0.857</td>\n",
       "      <td>0.735</td>\n",
       "      <td>0.857</td>\n",
       "      <td>0.791</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>15.000</td>\n",
       "      <td>20.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>20.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>1.609</td>\n",
       "      <td>0.857</td>\n",
       "      <td>0.735</td>\n",
       "      <td>0.857</td>\n",
       "      <td>0.791</td>\n",
       "      <td>nan</td>\n",
       "      <td>2.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    num_trees  num_features  num_sample_rows  max_tree_depth  \\\n",
       "0      10.000        -1.000           -1.000          20.000   \n",
       "1      10.000        -1.000           -1.000          20.000   \n",
       "2      10.000        -1.000           -1.000          20.000   \n",
       "3      10.000        -1.000           -1.000          20.000   \n",
       "4      10.000        -1.000           -1.000          20.000   \n",
       "5      10.000        -1.000           -1.000          20.000   \n",
       "6      10.000        -1.000           -1.000          20.000   \n",
       "7      10.000        -1.000           -1.000          20.000   \n",
       "8      10.000        -1.000           -1.000          20.000   \n",
       "9      10.000        -1.000           -1.000          20.000   \n",
       "10     15.000        20.000           -1.000          20.000   \n",
       "\n",
       "    min_split_samples  log_loss  class_accuracy  precision  recall  f1-score  \\\n",
       "0               5.000     1.609           0.867      0.751   0.867     0.805   \n",
       "1               5.000     1.609           0.733      0.538   0.733     0.621   \n",
       "2               5.000     1.609           0.667      0.444   0.667     0.533   \n",
       "3               5.000     1.609           0.800      0.640   0.800     0.711   \n",
       "4               5.000     1.609           0.800      0.640   0.800     0.711   \n",
       "5               5.000     1.609           0.400      0.160   0.400     0.229   \n",
       "6               5.000     1.609           0.667      0.444   0.667     0.533   \n",
       "7               5.000     1.609           0.733      0.538   0.733     0.621   \n",
       "8               5.000     1.609           0.667      0.444   0.667     0.533   \n",
       "9               5.000     1.609           0.857      0.735   0.857     0.791   \n",
       "10              2.000     1.609           0.857      0.735   0.857     0.791   \n",
       "\n",
       "    support  trial_num  \n",
       "0       nan      1.000  \n",
       "1       nan      1.000  \n",
       "2       nan      1.000  \n",
       "3       nan      1.000  \n",
       "4       nan      1.000  \n",
       "5       nan      1.000  \n",
       "6       nan      1.000  \n",
       "7       nan      1.000  \n",
       "8       nan      1.000  \n",
       "9       nan      1.000  \n",
       "10      nan      2.000  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e.experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trial 1 Notes:\n",
    "\n",
    "These settings were default. Since we have 5 classes, the range of log loss is between 0 and 1.6, meaning our log loss is pretty bad and that we're very likely to misclassify. The accuracy seems good, but that's only because class 1 has a higher chance to appear in general as it appears ~70% of the time. As we see in iteration 7, accuracy and precision are _.533_ and _.284_  respectively. That iteration probably had more diverse labels than usual, but the model only got it right $\\frac{1}{4}^{th}%$ of the time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_7_train, t_7_test  = e.experiment_data[6]\n",
    "\n",
    "t_7_train_g = merge(t_7_train, labels).groupby(\"label\")\n",
    "t_7_test_g = merge(t_7_test, labels).groupby(\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "1   0.724\n",
       "2   0.097\n",
       "3   0.052\n",
       "4   0.104\n",
       "5   0.022\n",
       "Name: f0, dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_7_train_g.count().apply(lambda x: x / t_7_train.shape[0]).T.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x10c987898>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10c987c50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "e.experiments.sort_values([\"recall\", \"precision\"]).plot(\"recall\", \"precision\", \"line\", figsize=(5,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very odd that precision and recall move in step.. Not sure what that means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "log_loss         1.609\n",
       "class_accuracy   0.732\n",
       "precision        0.552\n",
       "recall           0.732\n",
       "f1-score         0.625\n",
       "support            nan\n",
       "dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e.experiments[e.performance_measures()].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, it seems like the classifier is stuck on the imbalance of classes in the dataset. We have close to 0 variance but lots of bias. To combat this, we'll add more & deeper trees in hopes that the model will pick up on more variance, train some trees only on data that have the imbalanced classes, and use less data to train the model with in order to reduce the extent to which a single class and dominate the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Experiment at 0x10c0a66d8>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e.tweak(\"num_trees\", 15).tweak(\"min_split_samples\", 2).tweak(\"num_features\", 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running trail #11\n",
      "------------------------------------\n",
      " \n",
      "*************************\n",
      " \n",
      "Running fold 1 of 10\n",
      " \n",
      "*************************\n",
      " \n",
      "fold 1 took 43.15527701377869s\n",
      " \n",
      "*************************\n",
      " \n",
      "Running fold 2 of 10\n",
      " \n",
      "*************************\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/python3/3.5.0/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 2 took 41.536792039871216s\n",
      " \n",
      "*************************\n",
      " \n",
      "Running fold 3 of 10\n",
      " \n",
      "*************************\n",
      " \n",
      "fold 3 took 47.262762784957886s\n",
      " \n",
      "*************************\n",
      " \n",
      "Running fold 4 of 10\n",
      " \n",
      "*************************\n",
      " \n",
      "fold 4 took 56.707643032073975s\n",
      " \n",
      "*************************\n",
      " \n",
      "Running fold 5 of 10\n",
      " \n",
      "*************************\n",
      " \n",
      "fold 5 took 43.702346086502075s\n",
      " \n",
      "*************************\n",
      " \n",
      "Running fold 6 of 10\n",
      " \n",
      "*************************\n",
      " \n",
      "fold 6 took 47.30037593841553s\n",
      " \n",
      "*************************\n",
      " \n",
      "Running fold 7 of 10\n",
      " \n",
      "*************************\n",
      " \n",
      "fold 7 took 29.99281096458435s\n",
      " \n",
      "*************************\n",
      " \n",
      "Running fold 8 of 10\n",
      " \n",
      "*************************\n",
      " \n",
      "fold 8 took 47.83272194862366s\n",
      " \n",
      "*************************\n",
      " \n",
      "Running fold 9 of 10\n",
      " \n",
      "*************************\n",
      " \n",
      "fold 9 took 42.6654269695282s\n",
      " \n",
      "*************************\n",
      " \n",
      "Running fold 10 of 10\n",
      " \n",
      "*************************\n",
      " \n",
      "fold 10 took 40.48732113838196s\n",
      " \n",
      "Warning:e is <__main__.Experiment object at 0x10c0a66d8>\n",
      "Proper storage of interactively declared classes (or instances\n",
      "of those classes) is not possible! Only instances\n",
      "of classes in real modules on file system can be %store'd.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "e.run_trial()\n",
    "%store e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2_results, t2_train_test =  e.trial_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_trees</th>\n",
       "      <th>num_features</th>\n",
       "      <th>num_sample_rows</th>\n",
       "      <th>max_tree_depth</th>\n",
       "      <th>min_split_samples</th>\n",
       "      <th>log_loss</th>\n",
       "      <th>class_accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "      <th>trial_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>15.000</td>\n",
       "      <td>12.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>20.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>1.609</td>\n",
       "      <td>0.733</td>\n",
       "      <td>0.538</td>\n",
       "      <td>0.733</td>\n",
       "      <td>0.621</td>\n",
       "      <td>nan</td>\n",
       "      <td>2.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>15.000</td>\n",
       "      <td>12.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>20.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>1.609</td>\n",
       "      <td>0.800</td>\n",
       "      <td>0.640</td>\n",
       "      <td>0.800</td>\n",
       "      <td>0.711</td>\n",
       "      <td>nan</td>\n",
       "      <td>2.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>15.000</td>\n",
       "      <td>12.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>20.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>1.609</td>\n",
       "      <td>0.733</td>\n",
       "      <td>0.538</td>\n",
       "      <td>0.733</td>\n",
       "      <td>0.621</td>\n",
       "      <td>nan</td>\n",
       "      <td>2.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>15.000</td>\n",
       "      <td>12.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>20.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>1.609</td>\n",
       "      <td>0.533</td>\n",
       "      <td>0.284</td>\n",
       "      <td>0.533</td>\n",
       "      <td>0.371</td>\n",
       "      <td>nan</td>\n",
       "      <td>2.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15.000</td>\n",
       "      <td>12.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>20.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>1.609</td>\n",
       "      <td>0.733</td>\n",
       "      <td>0.538</td>\n",
       "      <td>0.733</td>\n",
       "      <td>0.621</td>\n",
       "      <td>nan</td>\n",
       "      <td>2.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15.000</td>\n",
       "      <td>12.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>20.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>1.609</td>\n",
       "      <td>0.733</td>\n",
       "      <td>0.538</td>\n",
       "      <td>0.733</td>\n",
       "      <td>0.621</td>\n",
       "      <td>nan</td>\n",
       "      <td>2.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>15.000</td>\n",
       "      <td>12.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>20.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>1.609</td>\n",
       "      <td>0.800</td>\n",
       "      <td>0.640</td>\n",
       "      <td>0.800</td>\n",
       "      <td>0.711</td>\n",
       "      <td>nan</td>\n",
       "      <td>2.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>15.000</td>\n",
       "      <td>12.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>20.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>1.609</td>\n",
       "      <td>0.533</td>\n",
       "      <td>0.284</td>\n",
       "      <td>0.533</td>\n",
       "      <td>0.371</td>\n",
       "      <td>nan</td>\n",
       "      <td>2.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>15.000</td>\n",
       "      <td>12.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>20.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>1.609</td>\n",
       "      <td>0.800</td>\n",
       "      <td>0.640</td>\n",
       "      <td>0.800</td>\n",
       "      <td>0.711</td>\n",
       "      <td>nan</td>\n",
       "      <td>2.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>15.000</td>\n",
       "      <td>12.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>20.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>1.609</td>\n",
       "      <td>0.786</td>\n",
       "      <td>0.617</td>\n",
       "      <td>0.786</td>\n",
       "      <td>0.691</td>\n",
       "      <td>nan</td>\n",
       "      <td>2.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    num_trees  num_features  num_sample_rows  max_tree_depth  \\\n",
       "10     15.000        12.000           -1.000          20.000   \n",
       "11     15.000        12.000           -1.000          20.000   \n",
       "12     15.000        12.000           -1.000          20.000   \n",
       "13     15.000        12.000           -1.000          20.000   \n",
       "14     15.000        12.000           -1.000          20.000   \n",
       "15     15.000        12.000           -1.000          20.000   \n",
       "16     15.000        12.000           -1.000          20.000   \n",
       "17     15.000        12.000           -1.000          20.000   \n",
       "18     15.000        12.000           -1.000          20.000   \n",
       "19     15.000        12.000           -1.000          20.000   \n",
       "\n",
       "    min_split_samples  log_loss  class_accuracy  precision  recall  f1-score  \\\n",
       "10              2.000     1.609           0.733      0.538   0.733     0.621   \n",
       "11              2.000     1.609           0.800      0.640   0.800     0.711   \n",
       "12              2.000     1.609           0.733      0.538   0.733     0.621   \n",
       "13              2.000     1.609           0.533      0.284   0.533     0.371   \n",
       "14              2.000     1.609           0.733      0.538   0.733     0.621   \n",
       "15              2.000     1.609           0.733      0.538   0.733     0.621   \n",
       "16              2.000     1.609           0.800      0.640   0.800     0.711   \n",
       "17              2.000     1.609           0.533      0.284   0.533     0.371   \n",
       "18              2.000     1.609           0.800      0.640   0.800     0.711   \n",
       "19              2.000     1.609           0.786      0.617   0.786     0.691   \n",
       "\n",
       "    support  trial_num  \n",
       "10      nan      2.000  \n",
       "11      nan      2.000  \n",
       "12      nan      2.000  \n",
       "13      nan      2.000  \n",
       "14      nan      2.000  \n",
       "15      nan      2.000  \n",
       "16      nan      2.000  \n",
       "17      nan      2.000  \n",
       "18      nan      2.000  \n",
       "19      nan      2.000  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x10ce30ef0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10ca80cc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "t2_results.sort_values([\"recall\", \"precision\"]).plot(\"recall\", \"precision\", \"line\", figsize=(5,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trial 2 Notes:\n",
    "Welp - that didn't work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
