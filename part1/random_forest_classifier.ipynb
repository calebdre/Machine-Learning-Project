{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%store -r\n",
    "\n",
    "from time import time\n",
    "from math import sqrt, floor\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "import pandas as pd\n",
    "from IPython.core.debugger import set_trace\n",
    "import pickle\n",
    "import os\n",
    "from rfc import RandomForest\n",
    "pd.options.display.float_format = '{:.3f}'.format\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use = \"default\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.debugger import set_trace\n",
    "train = pd.read_csv(\"cleaned_testData1.csv\")\n",
    "labels = pd.read_csv(\"cleaned_trainLabel1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.drop(train.columns[0], axis=1)\n",
    "labels = labels.drop(labels.columns[0], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file = \"model.binary\"\n",
    "train_file = \"model-train.csv\"\n",
    "test_file = \"model-test.csv\"\n",
    "\n",
    "def save_model(model, train_set, test_set):\n",
    "    with open(model_file, \"wb\") as f:\n",
    "        pickle.dump(model, f)\n",
    "    train_set.to_csv(train_file)\n",
    "    test_set.to_csv(test_file)\n",
    "\n",
    "def read_model():\n",
    "    if os.path.exists(\"model.binary\"):\n",
    "        with open(\"model.binary\", \"rb\") as f:\n",
    "            model = pickle.load(f)\n",
    "        \n",
    "        return pd.read_csv(train_file), pd.read_csv(test_file), model\n",
    "    else:\n",
    "        False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(df, labels):\n",
    "    return labels.merge(df, left_index=True,right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mprint(*args):\n",
    "    for arg in args:\n",
    "        print(arg)\n",
    "        print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---  \n",
    "\n",
    "# Model Building\n",
    "\n",
    "## Which algorithm to use?\n",
    "We'll use a **random forest classifier** (rfc) with bootstrapping and feature bagging optimizations because:\n",
    "- ease of implementation\n",
    "- rfcs handle multi-class predictions well without more additional effort\n",
    "- works well with high dimensional data\n",
    "- we'll choose use random forest as opposed to boosted trees since we have highly dimensional data\n",
    "- with a reasonably high probability, can be used with the other datasets for this project since the algorithm is very robust\n",
    "\n",
    "## The Algorithm\n",
    "We'll use the CART algorithm for splitting since we have continuous data.  \n",
    "  \n",
    "[Full example](https://machinelearningmastery.com/classification-and-regression-trees-for-machine-learning/)  \n",
    "  \n",
    "Steps:\n",
    "1. Initialize Tree\n",
    "2. For each column, calc best split across all rows based using gini impurity score - [exmplanation](https://en.wikipedia.org/wiki/Decision_tree_learning#Gini_impurity) | [exmaple](https://www.researchgate.net/post/How_to_compute_impurity_using_Gini_Index) | [useful blog](http://dni-institute.in/blogs/cart-algorithm-for-decision-tree/)\n",
    "3. Split the dataset based on the split condition with the highest gini score and add both sets as leaves on a tree node. The node represents a decision point, that being the condition with the highest gini score.\n",
    "3. Repeat 2 & 3 until an arbitrary minimum number of rows are left\n",
    "4. Prune tree\n",
    "\n",
    "ideas:\n",
    "- instead of using the raw values, categorize the numbers as # of stds away from mean\n",
    "- > Alternatively, the random forest can apply weight concept for considering the impact of result from any decision tree. Tree with high error rate are given low weight value and vise versa. This would increase the decision impact of trees with low error rate - [medium post](https://medium.com/machine-learning-101/chapter-5-random-forest-classifier-56dc7425c3e1)\n",
    "- [parameters to  tune](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)\n",
    "https://www.analyticsvidhya.com/blog/2016/04/complete-tutorial-tree-based-modeling-scratch-in-python/\n",
    "- https://www.analyticsvidhya.com/blog/2015/06/tuning-random-forest-model/\n",
    "- https://stats.stackexchange.com/questions/260460/optimization-of-a-random-forest-model\n",
    "- https://followthedata.wordpress.com/2012/06/02/practical-advice-for-machine-learning-bias-variance/\n",
    "- https://towardsdatascience.com/beyond-accuracy-precision-and-recall-3da06bea9f6c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TEST\n",
    "# def calc_best_gini_split(df, labels):\n",
    "#     total_rows = df.shape[0]\n",
    "    \n",
    "#     def calc(col_df):\n",
    "#         def g(item):\n",
    "#             split1 = col_df[item > col_df]\n",
    "#             split2 = col_df[item < col_df]\n",
    "            \n",
    "#             s1_grouped = labels.merge(pd.DataFrame(split1), left_index=True, right_index=True).groupby(\"label\")\n",
    "#             s2_grouped = labels.merge(pd.DataFrame(split2), left_index=True, right_index=True).groupby(\"label\")\n",
    "            \n",
    "#             s1_group_count = s1_grouped.count()\n",
    "#             s2_group_count = s2_grouped.count()\n",
    "            \n",
    "#             s1_cost = (1 - np.power(s1_group_count / s1_group_count.sum(), 2).sum(axis=0)) * (split1.shape[0] / total_rows)\n",
    "#             s2_cost = (1 - np.power(s2_group_count / s2_group_count.sum(), 2).sum(axis=0)) * (split2.shape[0] / total_rows)\n",
    "\n",
    "#             return (s1_cost + s2_cost).iloc[0]\n",
    "        \n",
    "#         # minimum cost for particular columns        \n",
    "#         costs = col_df.apply(g)\n",
    "#         min_cost = costs.min()\n",
    "#         print(costs.shape, costs.idxmin(), costs, df)\n",
    "#         min_cost_val = costs.iloc[costs.idxmin()]\n",
    "        \n",
    "#         return pd.Series({\"cost\": min_cost, \"column\": col_df.name, \"val\": min_cost_val})\n",
    "    \n",
    "#     splits = df.apply(calc, axis=0)\n",
    "    \n",
    "#     # minimum cost for whole set\n",
    "#     transposed = splits.T\n",
    "#     transposed[\"cost\"] = transposed[\"cost\"].astype(np.float32)\n",
    "#     best_split = splits[transposed[\"cost\"].idxmin()]\n",
    "    \n",
    "#     return best_split[\"column\"], best_split[\"val\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t = time()\n",
    "# calc_best_gini_split(train.iloc[:, 0:70], labels)\n",
    "# time() - t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t = time()\n",
    "# calc_best_gini_split(train.iloc[:, 0:70], labels)\n",
    "# time() - t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_row = -5\n",
    "# t = train.iloc[:, 0:15]\n",
    "\n",
    "# DecisionTree().grow(t, labels).predict(t.iloc[test_row]), labels.iloc[-5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dft = train.iloc[:, 0:50]\n",
    "# rf = RandomForest()\n",
    "# rf.grow(dft, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_row = -5\n",
    "# rf.predict(train.iloc[test_row]), labels.iloc[test_row]\n",
    "# merge(dft, labels).groupby(\"label\").describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---  \n",
    "# Model Training & Tuning\n",
    "## Context\n",
    "Now that we have our classifier, let's think about how we're going to train the model. \n",
    "\n",
    "We'll also measure performance through [precision](https://en.wikipedia.org/wiki/Precision_and_recall) & [recall](https://towardsdatascience.com/beyond-accuracy-precision-and-recall-3da06bea9f6c) - it tells us, for each class, how well the model identifies all cases of that class (recall) and how well it can correctly classify those cases (precision). From wikipedia:\n",
    "> Suppose a computer program for recognizing dogs in photographs identifies eight dogs in a picture containing 12 dogs and some cats. Of the eight dogs identified, five actually are dogs (true positives), while the rest are cats (false positives). The program's precision is 5/8 while its recall is 5/12.\n",
    "\n",
    "![precision & recall formulas](https://cdn-images-1.medium.com/max/2000/1*6NkN_LINs2erxgVJ9rkpUA.png)\n",
    "We can use the [f1 score](https://en.wikipedia.org/wiki/F1_score) to maximize precision and recall when testing different models.  \n",
    "![f1 score formula](https://cdn-images-1.medium.com/max/1600/1*UJxVqLnbSj42eRhasKeLOA.png)\n",
    "\n",
    "Recall and precision seem to be very related to bias and variance of the model, so we can maximize the f1 score by tuning the model to affect these.\n",
    "#### Minimizing bias\n",
    "- use new/different features\n",
    "- increase the size of the trees (increases variance)\n",
    "- increase the number of trees in the forest\n",
    "\n",
    "#### Minimizing variance\n",
    "- decrease the number of features\n",
    "    + probably want to aim to features that are correlated and/or collapse the overall number of features through PCA\n",
    "- use more data for each tree  \n",
    "\n",
    "  \n",
    "Beware: too much completixy is bad & not enough complexity is also bad  \n",
    "![bias variance tradeoff](http://scott.fortmann-roe.com/docs/docs/BiasVariance/biasvariance.png)  \n",
    "  \n",
    "\n",
    "#### Stability\n",
    "We need to make sure to train the classifier on as many data points as possible while also leaving enough to test to reliably tell how well the classifier actually performs. We'll use [k-fold cross validation](https://www.analyticsvidhya.com/blog/2015/11/improve-model-performance-cross-validation-in-python-r/):  \n",
    "  \n",
    "> 1. Randomly split your entire dataset into k ”folds”.\n",
    "2. For each k folds in your dataset, build your model on k – 1 folds of the data set. Then, test the model to check the effectiveness for kth fold.\n",
    "3. Record the error you see on each of the predictions.\n",
    "4. Repeat this until each of the k folds has served as the test set.\n",
    "\n",
    "## Procedure\n",
    "1. Record and save an input configuration for the random forest\n",
    "1. Separate data into k folds\n",
    "2. For each fold *k*: \n",
    "    1. train the classifier on k-1 folds\n",
    "    2. predict the k-th fold\n",
    "    3. measure the: accuracy, [logarithmic](http://wiki.fast.ai/index.php/Log_Loss) [loss](https://towardsdatascience.com/metrics-to-evaluate-your-machine-learning-algorithm-f10ba6e38234#f217), recall, precision, and f1-score\n",
    "3. Record the performance measures & associate it with the input configuration\n",
    "3. Evaluate the overall performance difference across all configurations\n",
    "4. Change **at most** 1 variable from the input configuration that optimizes perfomance & repeat steps 1-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Experiment:\n",
    "    def __init__(self, df, labels, model_class, model_config_names, model_config_init_values, k_folds=10):        \n",
    "        self.data = df\n",
    "        self.labels = labels\n",
    "        self.label_values = labels[\"label\"].unique()\n",
    "        self.model_class = model_class\n",
    "        self.model_config_names = model_config_names\n",
    "        \n",
    "        self.init_experiments(model_config_names, model_config_init_values)\n",
    "        \n",
    "        self.k_folds = k_folds\n",
    "    \n",
    "    def tweak(self, parameter, new_value):\n",
    "        self.current_experiment()[parameter] = new_value\n",
    "        return self\n",
    "    \n",
    "    def run_trial(self):\n",
    "        exp_num = self.experiments.shape[0]\n",
    "        mprint(\"Running experiment #{}\\n------------------------------------\".format(exp_num))\n",
    "\n",
    "        performance_results = []\n",
    "        folds, labels = self.split_k_folds()\n",
    "        \n",
    "        config = self.current_model_config()\n",
    "        labels = pd.DataFrame(labels)\n",
    "\n",
    "        for i, test_train in enumerate(folds):\n",
    "            mprint(\n",
    "                \"*************************\",\n",
    "                \"Running fold {} of {}\".format(i+1, self.k_folds),\n",
    "                \"*************************\"\n",
    "            )\n",
    "            \n",
    "            model = self.model()\n",
    "\n",
    "            model.train(test_train[1], pd.DataFrame(labels), *config)\n",
    "            performance = self.measure_performance(model, merge(test_train[0], labels), self.label_values)\n",
    "            \n",
    "            self.record_trial(performance)\n",
    "            self.show_trial_results()\n",
    "    \n",
    "    def show_trial_results(self, trial_num=None):\n",
    "        if trial_num == None:\n",
    "            trial = self.prev_experiment()\n",
    "        else:\n",
    "            trial = self.experiments.iloc[trial_num]\n",
    "        \n",
    "        mprint(trial)\n",
    "    \n",
    "#<--------  PRIVATE METHODS -------->\n",
    "    def current_experiment(self):\n",
    "        return self.experiments.loc[self.experiments.shape[0]-1]\n",
    "    \n",
    "    def current_model_config(self):\n",
    "        return self.current_experiment()[self.model_config_names].astype(\"int32\").values\n",
    "        \n",
    "    def prev_experiment(self):\n",
    "        return self.experiments.loc[self.experiments.shape[0]-2]\n",
    "    \n",
    "    def model(self):\n",
    "        return self.model_class()\n",
    "    \n",
    "    def init_experiments(self, config_names, config_values):\n",
    "        cols = config_names + self.performance_measures()\n",
    "        first_row = config_values + [np.nan for i in config_values]\n",
    "        \n",
    "        self.experiments = pd.DataFrame(columns=cols)\n",
    "        self.experiments.loc[0] = first_row\n",
    "        self.experiments[config_names] = self.experiments[config_names].fillna(-1)\n",
    "    \n",
    "    def record_trial(self, results):\n",
    "        for key in results:\n",
    "            self.current_experiment()[key] = results[key]\n",
    "        \n",
    "        self.experiments.loc[self.experiments.shape[0]] = self.current_experiment\n",
    "    \n",
    "    def split_k_folds(self):\n",
    "        splitter = int(np.ceil(self.data.shape[0] / self.k_folds))\n",
    "        df = shuffle(merge(self.data, self.labels))\n",
    "        labels = df.pop(\"label\")\n",
    "\n",
    "        folds = []\n",
    "        for i in range(1, self.k_folds):\n",
    "            train = df.iloc[(i-1) * splitter: i * splitter]\n",
    "            test = df.iloc[np.r_[0:(i-1) * splitter, i*splitter: df.shape[0]]]\n",
    "            folds.append((train, test))\n",
    "\n",
    "        return folds, labels\n",
    "    \n",
    "    def performance_measures(self):\n",
    "        return [\"log_loss\", \"class_accuracy\", \"precision\", \"recall\", \"fscore\"]\n",
    "    \n",
    "    def measure_performance(self, model, test_set, label_values=None):\n",
    "        test_labels = test_set.pop(\"label\")\n",
    "        if label_values is None:\n",
    "            label_values = test_labels.unique()\n",
    "\n",
    "        predictions = test_set.apply(lambda row: model.predict(row), axis=1)\n",
    "        precision, recall, fscore, support = score(test_labels, predictions, average='weighted')\n",
    "\n",
    "        lvs = [[1 if p == 1 else 0 for l in label_values] for p in predictions]\n",
    "        return {\n",
    "            \"log_loss\" : log_loss(test_labels, lvs, normalize=True, labels=label_values),\n",
    "            \"class_accuracy\": accuracy_score(test_labels, predictions, normalize=True),\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1-score\": fscore\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_config_names = [\"num_trees\", \"num_features\", \"num_sample_rows\", \"max_tree_depth\", \"min_split_samples\"]\n",
    "init_config_values = [   10,          None,           None,              20,                5] # default settings = initial settings\n",
    "\n",
    "e = Experiment(train, labels, RandomForest, init_config_names, init_config_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trial #1: Defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment #1\n",
      "----------------------------------------------------------\n",
      " \n",
      "*************************\n",
      " \n",
      "Running 1 of 10 folds\n",
      " \n",
      "*************************\n",
      " \n",
      "\n",
      "*** Creating tree #1 ***\n",
      "\n",
      "parsing @ depth 1\n",
      "classified f184 at depth 1 to be in class 1\n",
      "\n",
      "*** Creating tree #2 ***\n",
      "\n",
      "parsing @ depth 1\n",
      "classified f564 at depth 1 to be in class 1\n",
      "\n",
      "*** Creating tree #3 ***\n",
      "\n",
      "parsing @ depth 1\n",
      "classified f457 at depth 1 to be in class 1\n",
      "\n",
      "*** Creating tree #4 ***\n",
      "\n",
      "parsing @ depth 1\n",
      "classified f642 at depth 1 to be in class 1\n",
      "\n",
      "*** Creating tree #5 ***\n",
      "\n",
      "parsing @ depth 1\n",
      "classified f88 at depth 1 to be in class 1\n",
      "\n",
      "*** Creating tree #6 ***\n",
      "\n",
      "parsing @ depth 1\n",
      "classified f20 at depth 1 to be in class 1\n",
      "\n",
      "*** Creating tree #7 ***\n",
      "\n",
      "parsing @ depth 1\n",
      "parsing @ depth 2\n",
      "classified f398 at depth 2 to be in class 4\n",
      "parsing @ depth 2\n",
      "classified f177 at depth 2 to be in class 1\n",
      "Done with depth 1\n",
      "\n",
      "*** Creating tree #8 ***\n",
      "\n",
      "parsing @ depth 1\n",
      "classified f63 at depth 1 to be in class 1\n",
      "\n",
      "*** Creating tree #9 ***\n",
      "\n",
      "parsing @ depth 1\n",
      "classified f607 at depth 1 to be in class 1\n",
      "\n",
      "*** Creating tree #10 ***\n",
      "\n",
      "parsing @ depth 1\n",
      "classified f607 at depth 1 to be in class 1\n",
      "num_trees           10.000\n",
      "num_features        -1.000\n",
      "num_sample_rows     -1.000\n",
      "max_tree_depth      20.000\n",
      "min_split_samples    5.000\n",
      "log_loss             1.609\n",
      "class_accuracy       0.733\n",
      "precision            0.538\n",
      "recall               0.733\n",
      "fscore                 NaN\n",
      "Name: 0, dtype: object\n",
      " \n",
      "*************************\n",
      " \n",
      "Running 2 of 10 folds\n",
      " \n",
      "*************************\n",
      " \n",
      "\n",
      "*** Creating tree #1 ***\n",
      "\n",
      "parsing @ depth 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/python3/3.5.0/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classified f82 at depth 1 to be in class 1\n",
      "\n",
      "*** Creating tree #2 ***\n",
      "\n",
      "parsing @ depth 1\n",
      "classified f642 at depth 1 to be in class 1\n",
      "\n",
      "*** Creating tree #3 ***\n",
      "\n",
      "parsing @ depth 1\n",
      "classified f122 at depth 1 to be in class 1\n",
      "\n",
      "*** Creating tree #4 ***\n",
      "\n",
      "parsing @ depth 1\n",
      "classified f374 at depth 1 to be in class 1\n",
      "\n",
      "*** Creating tree #5 ***\n",
      "\n",
      "parsing @ depth 1\n",
      "classified f570 at depth 1 to be in class 1\n",
      "\n",
      "*** Creating tree #6 ***\n",
      "\n",
      "parsing @ depth 1\n",
      "classified f642 at depth 1 to be in class 1\n",
      "\n",
      "*** Creating tree #7 ***\n",
      "\n",
      "parsing @ depth 1\n",
      "classified f184 at depth 1 to be in class 1\n",
      "\n",
      "*** Creating tree #8 ***\n",
      "\n",
      "parsing @ depth 1\n",
      "classified f607 at depth 1 to be in class 1\n",
      "\n",
      "*** Creating tree #9 ***\n",
      "\n",
      "parsing @ depth 1\n",
      "classified f402 at depth 1 to be in class 1\n",
      "\n",
      "*** Creating tree #10 ***\n",
      "\n",
      "parsing @ depth 1\n",
      "classified f488 at depth 1 to be in class 1\n",
      "num_trees            <bound method Experiment.current_experiment of...\n",
      "num_features         <bound method Experiment.current_experiment of...\n",
      "num_sample_rows      <bound method Experiment.current_experiment of...\n",
      "max_tree_depth       <bound method Experiment.current_experiment of...\n",
      "min_split_samples    <bound method Experiment.current_experiment of...\n",
      "log_loss                                                         1.609\n",
      "class_accuracy                                                   0.733\n",
      "precision                                                        0.538\n",
      "recall                                                           0.733\n",
      "fscore               <bound method Experiment.current_experiment of...\n",
      "Name: 1, dtype: object\n",
      " \n",
      "*************************\n",
      " \n",
      "Running 3 of 10 folds\n",
      " \n",
      "*************************\n",
      " \n",
      "\n",
      "*** Creating tree #1 ***\n",
      "\n",
      "parsing @ depth 1\n",
      "parsing @ depth 2\n",
      "classified f271 at depth 2 to be in class 4\n",
      "parsing @ depth 2\n",
      "classified f84 at depth 2 to be in class 1\n",
      "Done with depth 1\n",
      "\n",
      "*** Creating tree #2 ***\n",
      "\n",
      "parsing @ depth 1\n",
      "classified f411 at depth 1 to be in class 1\n",
      "\n",
      "*** Creating tree #3 ***\n",
      "\n",
      "parsing @ depth 1\n",
      "classified f39 at depth 1 to be in class 1\n",
      "\n",
      "*** Creating tree #4 ***\n",
      "\n",
      "parsing @ depth 1\n",
      "classified f34 at depth 1 to be in class 1\n",
      "\n",
      "*** Creating tree #5 ***\n",
      "\n",
      "parsing @ depth 1\n",
      "classified f45 at depth 1 to be in class 1\n",
      "\n",
      "*** Creating tree #6 ***\n",
      "\n",
      "parsing @ depth 1\n",
      "classified f597 at depth 1 to be in class 1\n",
      "\n",
      "*** Creating tree #7 ***\n",
      "\n",
      "parsing @ depth 1\n",
      "parsing @ depth 2\n",
      "classified f45 at depth 2 to be in class 4\n",
      "parsing @ depth 2\n",
      "classified f45 at depth 2 to be in class 1\n",
      "Done with depth 1\n",
      "\n",
      "*** Creating tree #8 ***\n",
      "\n",
      "parsing @ depth 1\n",
      "classified f107 at depth 1 to be in class 1\n",
      "\n",
      "*** Creating tree #9 ***\n",
      "\n",
      "parsing @ depth 1\n",
      "classified f34 at depth 1 to be in class 1\n",
      "\n",
      "*** Creating tree #10 ***\n",
      "\n",
      "parsing @ depth 1\n",
      "classified f596 at depth 1 to be in class 1\n",
      "num_trees            <bound method Experiment.current_experiment of...\n",
      "num_features         <bound method Experiment.current_experiment of...\n",
      "num_sample_rows      <bound method Experiment.current_experiment of...\n",
      "max_tree_depth       <bound method Experiment.current_experiment of...\n",
      "min_split_samples    <bound method Experiment.current_experiment of...\n",
      "log_loss                                                         1.609\n",
      "class_accuracy                                                   0.667\n",
      "precision                                                        0.444\n",
      "recall                                                           0.667\n",
      "fscore               <bound method Experiment.current_experiment of...\n",
      "Name: 2, dtype: object\n",
      " \n",
      "*************************\n",
      " \n",
      "Running 4 of 10 folds\n",
      " \n",
      "*************************\n",
      " \n",
      "\n",
      "*** Creating tree #1 ***\n",
      "\n",
      "parsing @ depth 1\n",
      "classified f411 at depth 1 to be in class 1\n",
      "\n",
      "*** Creating tree #2 ***\n",
      "\n",
      "parsing @ depth 1\n",
      "classified f173 at depth 1 to be in class 1\n",
      "\n",
      "*** Creating tree #3 ***\n",
      "\n",
      "parsing @ depth 1\n",
      "classified f184 at depth 1 to be in class 1\n",
      "\n",
      "*** Creating tree #4 ***\n",
      "\n",
      "parsing @ depth 1\n",
      "parsing @ depth 2\n",
      "classified f329 at depth 2 to be in class 4\n",
      "parsing @ depth 2\n",
      "parsing @ depth 3\n",
      "classified f276 at depth 3 to be in class 1\n",
      "parsing @ depth 3\n",
      "parsing @ depth 4\n",
      "classified f329 at depth 4 to be in class 1\n",
      "parsing @ depth 4\n",
      "classified f329 at depth 4 to be in class 1\n",
      "Done with depth 3\n",
      "Done with depth 2\n",
      "Done with depth 1\n",
      "\n",
      "*** Creating tree #5 ***\n",
      "\n",
      "parsing @ depth 1\n",
      "classified f184 at depth 1 to be in class 1\n",
      "\n",
      "*** Creating tree #6 ***\n",
      "\n",
      "parsing @ depth 1\n",
      "classified f42 at depth 1 to be in class 1\n",
      "\n",
      "*** Creating tree #7 ***\n",
      "\n",
      "parsing @ depth 1\n",
      "classified f151 at depth 1 to be in class 1\n",
      "\n",
      "*** Creating tree #8 ***\n",
      "\n",
      "parsing @ depth 1\n",
      "parsing @ depth 2\n",
      "classified f79 at depth 2 to be in class 4\n",
      "parsing @ depth 2\n",
      "parsing @ depth 3\n",
      "classified f79 at depth 3 to be in class 1\n",
      "parsing @ depth 3\n",
      "parsing @ depth 4\n",
      "classified f79 at depth 4 to be in class 1\n",
      "parsing @ depth 4\n",
      "classified f79 at depth 4 to be in class 1\n",
      "Done with depth 3\n",
      "Done with depth 2\n",
      "Done with depth 1\n",
      "\n",
      "*** Creating tree #9 ***\n",
      "\n",
      "parsing @ depth 1\n",
      "classified f596 at depth 1 to be in class 1\n",
      "\n",
      "*** Creating tree #10 ***\n",
      "\n",
      "parsing @ depth 1\n",
      "classified f34 at depth 1 to be in class 1\n",
      "num_trees            <bound method Experiment.current_experiment of...\n",
      "num_features         <bound method Experiment.current_experiment of...\n",
      "num_sample_rows      <bound method Experiment.current_experiment of...\n",
      "max_tree_depth       <bound method Experiment.current_experiment of...\n",
      "min_split_samples    <bound method Experiment.current_experiment of...\n",
      "log_loss                                                         1.609\n",
      "class_accuracy                                                   0.733\n",
      "precision                                                        0.538\n",
      "recall                                                           0.733\n",
      "fscore               <bound method Experiment.current_experiment of...\n",
      "Name: 3, dtype: object\n",
      " \n",
      "*************************\n",
      " \n",
      "Running 5 of 10 folds\n",
      " \n",
      "*************************\n",
      " \n",
      "\n",
      "*** Creating tree #1 ***\n",
      "\n",
      "parsing @ depth 1\n",
      "classified f151 at depth 1 to be in class 1\n",
      "\n",
      "*** Creating tree #2 ***\n",
      "\n",
      "parsing @ depth 1\n",
      "classified f672 at depth 1 to be in class 1\n",
      "\n",
      "*** Creating tree #3 ***\n",
      "\n",
      "parsing @ depth 1\n",
      "classified f82 at depth 1 to be in class 1\n",
      "\n",
      "*** Creating tree #4 ***\n",
      "\n",
      "parsing @ depth 1\n",
      "classified f411 at depth 1 to be in class 1\n",
      "\n",
      "*** Creating tree #5 ***\n",
      "\n",
      "parsing @ depth 1\n",
      "classified f236 at depth 1 to be in class 1\n",
      "\n",
      "*** Creating tree #6 ***\n",
      "\n",
      "parsing @ depth 1\n",
      "classified f63 at depth 1 to be in class 1\n",
      "\n",
      "*** Creating tree #7 ***\n",
      "\n",
      "parsing @ depth 1\n",
      "classified f244 at depth 1 to be in class 1\n",
      "\n",
      "*** Creating tree #8 ***\n",
      "\n",
      "parsing @ depth 1\n",
      "classified f327 at depth 1 to be in class 1\n",
      "\n",
      "*** Creating tree #9 ***\n",
      "\n",
      "parsing @ depth 1\n",
      "parsing @ depth 2\n",
      "classified f414 at depth 2 to be in class 2\n",
      "parsing @ depth 2\n",
      "classified f414 at depth 2 to be in class 1\n",
      "Done with depth 1\n",
      "\n",
      "*** Creating tree #10 ***\n",
      "\n",
      "parsing @ depth 1\n",
      "classified f81 at depth 1 to be in class 1\n",
      "num_trees            <bound method Experiment.current_experiment of...\n",
      "num_features         <bound method Experiment.current_experiment of...\n",
      "num_sample_rows      <bound method Experiment.current_experiment of...\n",
      "max_tree_depth       <bound method Experiment.current_experiment of...\n",
      "min_split_samples    <bound method Experiment.current_experiment of...\n",
      "log_loss                                                         1.609\n",
      "class_accuracy                                                   0.533\n",
      "precision                                                        0.284\n",
      "recall                                                           0.533\n",
      "fscore               <bound method Experiment.current_experiment of...\n",
      "Name: 4, dtype: object\n",
      " \n",
      "*************************\n",
      " \n",
      "Running 6 of 10 folds\n",
      " \n",
      "*************************\n",
      " \n",
      "\n",
      "*** Creating tree #1 ***\n",
      "\n",
      "parsing @ depth 1\n",
      "classified f18 at depth 1 to be in class 1\n",
      "\n",
      "*** Creating tree #2 ***\n",
      "\n",
      "parsing @ depth 1\n",
      "classified f672 at depth 1 to be in class 1\n",
      "\n",
      "*** Creating tree #3 ***\n",
      "\n",
      "parsing @ depth 1\n",
      "classified f81 at depth 1 to be in class 1\n",
      "\n",
      "*** Creating tree #4 ***\n",
      "\n",
      "parsing @ depth 1\n",
      "classified f672 at depth 1 to be in class 1\n",
      "\n",
      "*** Creating tree #5 ***\n",
      "\n",
      "parsing @ depth 1\n",
      "classified f88 at depth 1 to be in class 1\n",
      "\n",
      "*** Creating tree #6 ***\n",
      "\n",
      "parsing @ depth 1\n",
      "classified f173 at depth 1 to be in class 1\n",
      "\n",
      "*** Creating tree #7 ***\n",
      "\n",
      "parsing @ depth 1\n",
      "classified f236 at depth 1 to be in class 1\n",
      "\n",
      "*** Creating tree #8 ***\n",
      "\n",
      "parsing @ depth 1\n",
      "classified f151 at depth 1 to be in class 1\n",
      "\n",
      "*** Creating tree #9 ***\n",
      "\n",
      "parsing @ depth 1\n",
      "classified f184 at depth 1 to be in class 1\n",
      "\n",
      "*** Creating tree #10 ***\n",
      "\n",
      "parsing @ depth 1\n",
      "classified f317 at depth 1 to be in class 1\n",
      "num_trees            <bound method Experiment.current_experiment of...\n",
      "num_features         <bound method Experiment.current_experiment of...\n",
      "num_sample_rows      <bound method Experiment.current_experiment of...\n",
      "max_tree_depth       <bound method Experiment.current_experiment of...\n",
      "min_split_samples    <bound method Experiment.current_experiment of...\n",
      "log_loss                                                         1.609\n",
      "class_accuracy                                                   0.733\n",
      "precision                                                        0.538\n",
      "recall                                                           0.733\n",
      "fscore               <bound method Experiment.current_experiment of...\n",
      "Name: 5, dtype: object\n",
      " \n",
      "*************************\n",
      " \n",
      "Running 7 of 10 folds\n",
      " \n",
      "*************************\n",
      " \n",
      "\n",
      "*** Creating tree #1 ***\n",
      "\n",
      "parsing @ depth 1\n",
      "classified f407 at depth 1 to be in class 1\n",
      "\n",
      "*** Creating tree #2 ***\n",
      "\n",
      "parsing @ depth 1\n",
      "classified f62 at depth 1 to be in class 1\n",
      "\n",
      "*** Creating tree #3 ***\n",
      "\n",
      "parsing @ depth 1\n",
      "classified f107 at depth 1 to be in class 1\n",
      "\n",
      "*** Creating tree #4 ***\n",
      "\n",
      "parsing @ depth 1\n",
      "classified f184 at depth 1 to be in class 1\n",
      "\n",
      "*** Creating tree #5 ***\n",
      "\n",
      "parsing @ depth 1\n",
      "classified f34 at depth 1 to be in class 1\n",
      "\n",
      "*** Creating tree #6 ***\n",
      "\n",
      "parsing @ depth 1\n",
      "classified f587 at depth 1 to be in class 1\n",
      "\n",
      "*** Creating tree #7 ***\n",
      "\n",
      "parsing @ depth 1\n",
      "classified f151 at depth 1 to be in class 1\n",
      "\n",
      "*** Creating tree #8 ***\n",
      "\n",
      "parsing @ depth 1\n",
      "classified f107 at depth 1 to be in class 1\n",
      "\n",
      "*** Creating tree #9 ***\n",
      "\n",
      "parsing @ depth 1\n",
      "classified f34 at depth 1 to be in class 1\n",
      "\n",
      "*** Creating tree #10 ***\n",
      "\n",
      "parsing @ depth 1\n",
      "classified f599 at depth 1 to be in class 1\n",
      "num_trees            <bound method Experiment.current_experiment of...\n",
      "num_features         <bound method Experiment.current_experiment of...\n",
      "num_sample_rows      <bound method Experiment.current_experiment of...\n",
      "max_tree_depth       <bound method Experiment.current_experiment of...\n",
      "min_split_samples    <bound method Experiment.current_experiment of...\n",
      "log_loss                                                         1.609\n",
      "class_accuracy                                                   0.667\n",
      "precision                                                        0.444\n",
      "recall                                                           0.667\n",
      "fscore               <bound method Experiment.current_experiment of...\n",
      "Name: 6, dtype: object\n",
      " \n",
      "*************************\n",
      " \n",
      "Running 8 of 10 folds\n",
      " \n",
      "*************************\n",
      " \n",
      "\n",
      "*** Creating tree #1 ***\n",
      "\n",
      "parsing @ depth 1\n",
      "classified f20 at depth 1 to be in class 1\n",
      "\n",
      "*** Creating tree #2 ***\n",
      "\n",
      "parsing @ depth 1\n",
      "parsing @ depth 2\n",
      "classified f392 at depth 2 to be in class 2\n",
      "parsing @ depth 2\n",
      "classified f392 at depth 2 to be in class 1\n",
      "Done with depth 1\n",
      "\n",
      "*** Creating tree #3 ***\n",
      "\n",
      "parsing @ depth 1\n",
      "parsing @ depth 2\n",
      "classified f619 at depth 2 to be in class 4\n",
      "parsing @ depth 2\n",
      "parsing @ depth 3\n",
      "classified f619 at depth 3 to be in class 1\n",
      "parsing @ depth 3\n",
      "classified f619 at depth 3 to be in class 1\n",
      "Done with depth 2\n",
      "Done with depth 1\n",
      "\n",
      "*** Creating tree #4 ***\n",
      "\n",
      "parsing @ depth 1\n",
      "classified f463 at depth 1 to be in class 1\n",
      "\n",
      "*** Creating tree #5 ***\n",
      "\n",
      "parsing @ depth 1\n",
      "classified f607 at depth 1 to be in class 1\n",
      "\n",
      "*** Creating tree #6 ***\n",
      "\n",
      "parsing @ depth 1\n",
      "classified f82 at depth 1 to be in class 1\n",
      "\n",
      "*** Creating tree #7 ***\n",
      "\n",
      "parsing @ depth 1\n",
      "classified f543 at depth 1 to be in class 1\n",
      "\n",
      "*** Creating tree #8 ***\n",
      "\n",
      "parsing @ depth 1\n",
      "parsing @ depth 2\n",
      "classified f368 at depth 2 to be in class 2\n",
      "parsing @ depth 2\n",
      "classified f29 at depth 2 to be in class 1\n",
      "Done with depth 1\n",
      "\n",
      "*** Creating tree #9 ***\n",
      "\n",
      "parsing @ depth 1\n",
      "classified f607 at depth 1 to be in class 1\n",
      "\n",
      "*** Creating tree #10 ***\n",
      "\n",
      "parsing @ depth 1\n",
      "classified f63 at depth 1 to be in class 1\n",
      "num_trees            <bound method Experiment.current_experiment of...\n",
      "num_features         <bound method Experiment.current_experiment of...\n",
      "num_sample_rows      <bound method Experiment.current_experiment of...\n",
      "max_tree_depth       <bound method Experiment.current_experiment of...\n",
      "min_split_samples    <bound method Experiment.current_experiment of...\n",
      "log_loss                                                         1.609\n",
      "class_accuracy                                                   0.667\n",
      "precision                                                        0.444\n",
      "recall                                                           0.667\n",
      "fscore               <bound method Experiment.current_experiment of...\n",
      "Name: 7, dtype: object\n",
      " \n",
      "*************************\n",
      " \n",
      "Running 9 of 10 folds\n",
      " \n",
      "*************************\n",
      " \n",
      "\n",
      "*** Creating tree #1 ***\n",
      "\n",
      "parsing @ depth 1\n",
      "classified f81 at depth 1 to be in class 1\n",
      "\n",
      "*** Creating tree #2 ***\n",
      "\n",
      "parsing @ depth 1\n",
      "classified f92 at depth 1 to be in class 1\n",
      "\n",
      "*** Creating tree #3 ***\n",
      "\n",
      "parsing @ depth 1\n",
      "classified f184 at depth 1 to be in class 1\n",
      "\n",
      "*** Creating tree #4 ***\n",
      "\n",
      "parsing @ depth 1\n",
      "classified f88 at depth 1 to be in class 1\n",
      "\n",
      "*** Creating tree #5 ***\n",
      "\n",
      "parsing @ depth 1\n",
      "classified f327 at depth 1 to be in class 1\n",
      "\n",
      "*** Creating tree #6 ***\n",
      "\n",
      "parsing @ depth 1\n",
      "classified f273 at depth 1 to be in class 1\n",
      "\n",
      "*** Creating tree #7 ***\n",
      "\n",
      "parsing @ depth 1\n",
      "classified f350 at depth 1 to be in class 1\n",
      "\n",
      "*** Creating tree #8 ***\n",
      "\n",
      "parsing @ depth 1\n",
      "classified f127 at depth 1 to be in class 1\n",
      "\n",
      "*** Creating tree #9 ***\n",
      "\n",
      "parsing @ depth 1\n",
      "classified f88 at depth 1 to be in class 1\n",
      "\n",
      "*** Creating tree #10 ***\n",
      "\n",
      "parsing @ depth 1\n",
      "classified f323 at depth 1 to be in class 1\n",
      "num_trees            <bound method Experiment.current_experiment of...\n",
      "num_features         <bound method Experiment.current_experiment of...\n",
      "num_sample_rows      <bound method Experiment.current_experiment of...\n",
      "max_tree_depth       <bound method Experiment.current_experiment of...\n",
      "min_split_samples    <bound method Experiment.current_experiment of...\n",
      "log_loss                                                         1.609\n",
      "class_accuracy                                                   0.933\n",
      "precision                                                        0.871\n",
      "recall                                                           0.933\n",
      "fscore               <bound method Experiment.current_experiment of...\n",
      "Name: 8, dtype: object\n",
      " \n"
     ]
    }
   ],
   "source": [
    "e.run_trial()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning:e is <__main__.Experiment object at 0x1121827f0>\n",
      "Proper storage of interactively declared classes (or instances\n",
      "of those classes) is not possible! Only instances\n",
      "of classes in real modules on file system can be %store'd.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "e.experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
